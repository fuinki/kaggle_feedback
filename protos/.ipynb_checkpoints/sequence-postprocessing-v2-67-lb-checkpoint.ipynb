{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sequence Post-Processing V2\n\nThis is a fork of Chris Deotte's [notebook](https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615). Thanks Chris!\n\nThis notebook contains an ensemble of 5 BigBird models trained on different subsets of the dataset. It also contains a post-processing pipeline that uses XGBoost to convert token-level predictions into sequence-level predictions. Because models like BigBird use a per-token loss function, and this competition uses a per-sequence scoring metric, post-processing token or word predictions into sequence predictions can yield signficant performance gains. Each model in the ensemble is trained on a randomly chosen 93% subset of the entire train set, with a distinct 7% subset left out from each training set.\n\nAdditionally, this notebook shows how this competition can be treated as 7 separate classification tasks. This is because, to my knowledge, the scoring metric does not take discourse type/class intersections into account. See this [discussion](https://www.kaggle.com/c/feedback-prize-2021/discussion/300001). Each class is predicted with a separate sequence classifier, and no measures are taken to prevent sequence intersections between classes.\n\nSecondary datasets are created for which each sample is a sub-sequence of words in a text. Class probability predictions from BigBird are used to generate features for each of these samples. A gradient boosting classifier is trained to predict the probability of a true positive for each discourse type. It was discovered that training these classifiers on out-of-sample predictions did not yield a significant performance increase, so they were trained on as much of the dataset as practically possible (50% at present). \n\nMost of the post-processing code and description is at the bottom of this notebook.\n\nCurrently this notebook uses\n\n* backbone BigBird  (with HuggingFace's head for TokenClassification)\n* NER formulation (with `is_split_into_words=False` tokenization)\n* 5 models trained on 93% of training data\n\nThis notebook uses many code cells from Raghavendrakotala's great notebook [here][1]. Don't forget to upvote Raghavendrakotala's notebook :-)\n\n[1]: https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\n[2]: https://www.kaggle.com/cdeotte/tensorflow-longformer-ner-cv-0-617\n[3]: https://arxiv.org/abs/2007.14062\n\n**Changes in V2:**\n* BigBird ensemble\n* larger sequence classifier training set\n* tuned XGBoost model\n* sequence position features\n* more inclusive heuristic constraints for considered word sub-sequences\n* resampling samples in sequence classification to 1:1\n* AdamW net training\n* more granular features (7 quantiles instead of 5) to represent distribution of class probabilites for a sequence\n* increased iterations (100) of sequence classifier probability threshold tuning\n* computation efficiency improvements","metadata":{"gradient":{"editing":false,"id":"da4ac754-386b-43f7-b04a-b5f36cdca216","kernelId":""}}},{"cell_type":"markdown","source":"# [Discussion: Ideas for Future Work](https://www.kaggle.com/c/feedback-prize-2021/discussion/308511)","metadata":{}},{"cell_type":"markdown","source":"# Configuration\nThis notebook can either train a new model or load a previously trained model (made from previous notebook version). Furthermore, this notebook can either create new NER labels or load existing NER labels (made from previous notebook version). In this notebook version, we will load model and load NER labels.\n\nAlso this notebook can load huggingface stuff (like tokenizers) from a Kaggle dataset, or download it from internet. (If it downloads from internet, you can then put it in a Kaggle dataset, so next time you can turn internet off).","metadata":{"gradient":{"editing":false,"id":"e0167440-e64f-45dd-89fd-961bba1bea34","kernelId":""}}},{"cell_type":"code","source":"import os, sys\n# DECLARE HOW MANY GPUS YOU WISH TO USE. \n# KAGGLE ONLY HAS 1, BUT OFFLINE, YOU CAN USE MORE\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #0,1,2,3 for four gpu\n\n# VERSION FOR SAVING MODEL WEIGHTS\nVER=26\n\n# IF VARIABLE IS NONE, THEN NOTEBOOK COMPUTES TOKENS\n# OTHERWISE NOTEBOOK LOADS TOKENS FROM PATH\nLOAD_TOKENS_FROM = '../input/py-bigbird-v26'\n\n# IF VARIABLE IS NONE, THEN NOTEBOOK TRAINS A NEW MODEL\n# OTHERWISE IT LOADS YOUR PREVIOUSLY TRAINED MODEL\nLOAD_MODEL_FROM = '../input/fullensemble'\n\n# Use the entire ensemble.\nENSEMBLE_IDS = list(range(5))\n\n# Setting Fold = None leaves out an arbitrary 10% of the dataset for sequence classifier training.\n# Setting Fold to one of [0,1,2,3,4] leaves out the portion of the dataset not trained on by the corresponding ensemble model.\n# 'half' leaves out an arbitrary 50%.\nFOLD = None\n\n# IF FOLLOWING IS NONE, THEN NOTEBOOK \n# USES INTERNET AND DOWNLOADS HUGGINGFACE \n# CONFIG, TOKENIZER, AND MODEL\nDOWNLOADED_MODEL_PATH = '../input/py-bigbird-v26' \n\nif DOWNLOADED_MODEL_PATH is None:\n    DOWNLOADED_MODEL_PATH = 'model'    \nMODEL_NAME = 'google/bigbird-roberta-base'\n\n# Tune the probability threshold for sequence classifiers to maximize F1\nTRAIN_SEQ_CLASSIFIERS = False\n\n# A cache of the BigBird predictions for the validation/sequence training set and the corresponding sequence dataset\nKAGGLE_CACHE = '../input/feedbackcache2'\n\ncache = 'cache'\ncacheExists = os.path.exists(cache)\nif not cacheExists:\n  os.makedirs(cache)","metadata":{"gradient":{"editing":false,"execution_count":2,"id":"8e783988-827d-4a32-9402-61603b89347b","kernelId":""},"execution":{"iopub.status.busy":"2022-01-30T19:56:39.499042Z","iopub.execute_input":"2022-01-30T19:56:39.499864Z","iopub.status.idle":"2022-01-30T19:56:39.52917Z","shell.execute_reply.started":"2022-01-30T19:56:39.499741Z","shell.execute_reply":"2022-01-30T19:56:39.528558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# skopt optimizer has a bug when scipy is installed with its default version\nif TRAIN_SEQ_CLASSIFIERS:\n    os.system('pip install --no-dependencies scipy==1.5.2 ')","metadata":{"gradient":{"editing":false,"execution_count":3,"id":"568dee32-38d0-4852-af82-c39e404c588a","kernelId":""},"execution":{"iopub.status.busy":"2022-01-30T19:56:39.53069Z","iopub.execute_input":"2022-01-30T19:56:39.530926Z","iopub.status.idle":"2022-01-30T19:56:39.536696Z","shell.execute_reply.started":"2022-01-30T19:56:39.530895Z","shell.execute_reply":"2022-01-30T19:56:39.536024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import cuda\nconfig = {'model_name': MODEL_NAME,   \n         'max_length': 1024,\n         'train_batch_size':4,\n         'valid_batch_size':4,\n         'epochs':5,\n         'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n         'max_grad_norm':10,\n         'device': 'cuda' if cuda.is_available() else 'cpu'}","metadata":{"gradient":{"editing":false,"execution_count":4,"id":"42571122-fde4-4d21-959f-2bd6327e8741","kernelId":""},"execution":{"iopub.status.busy":"2022-01-30T19:56:39.538917Z","iopub.execute_input":"2022-01-30T19:56:39.539448Z","iopub.status.idle":"2022-01-30T19:56:40.985624Z","shell.execute_reply.started":"2022-01-30T19:56:39.539376Z","shell.execute_reply":"2022-01-30T19:56:40.98483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# How To Submit PyTorch Without Internet\nMany people ask me, how do I submit PyTorch models without internet? With HuggingFace Transformer, it's easy. Just download the following 3 things (1) model weights, (2) tokenizer files, (3) config file, and upload them to a Kaggle dataset. Below shows code how to get the files from HuggingFace for Google's BigBird-base. But this same code can download any transformer, like for example roberta-base.","metadata":{"gradient":{"editing":false,"id":"a6cdc617-5a00-4369-8c04-a2f423e2fe26","kernelId":""}}},{"cell_type":"code","source":"from transformers import *\nif DOWNLOADED_MODEL_PATH == 'model':\n    os.mkdir('model')\n    \n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)\n    tokenizer.save_pretrained('model')\n\n    config_model = AutoConfig.from_pretrained(MODEL_NAME) \n    config_model.num_labels = 15\n    config_model.save_pretrained('model')\n\n    backbone = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, \n                                                               config=config_model)\n    backbone.save_pretrained('model')","metadata":{"gradient":{"editing":false,"execution_count":5,"id":"966f07cc-d67a-468c-9f3a-c65f8608309e","kernelId":""},"execution":{"iopub.status.busy":"2022-01-30T19:56:40.9871Z","iopub.execute_input":"2022-01-30T19:56:40.987591Z","iopub.status.idle":"2022-01-30T19:56:50.109508Z","shell.execute_reply.started":"2022-01-30T19:56:40.987554Z","shell.execute_reply":"2022-01-30T19:56:50.108694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data and Libraries\nIn addition to loading the train dataframe, we will load all the train and text files and save them in a dataframe.","metadata":{"gradient":{"editing":false,"id":"dabd82c1-80df-4783-a17e-f4728b2fcf27","kernelId":""}}},{"cell_type":"code","source":"import numpy as np, os \nfrom scipy import stats\nimport pandas as pd, gc \nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, AdamW\n\n\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nfrom sklearn.metrics import accuracy_score\nfrom torch.cuda import amp","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","gradient":{"editing":false,"execution_count":6,"id":"685d622c-38b1-4e57-99ff-c19b4d0e4b63","kernelId":""},"papermill":{"duration":3.432226,"end_time":"2021-12-21T12:12:07.26275","exception":false,"start_time":"2021-12-21T12:12:03.830524","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-30T19:56:50.112516Z","iopub.execute_input":"2022-01-30T19:56:50.113047Z","iopub.status.idle":"2022-01-30T19:56:50.118705Z","shell.execute_reply.started":"2022-01-30T19:56:50.113001Z","shell.execute_reply":"2022-01-30T19:56:50.11766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/feedback-prize-2021/train.csv')\nprint( train_df.shape )\ntrain_df.head()","metadata":{"gradient":{"execution_count":8,"id":"c48d29c7-0d7d-4782-ac7d-4f32e7586bac","kernelId":""},"papermill":{"duration":1.866495,"end_time":"2021-12-21T12:12:09.158087","exception":false,"start_time":"2021-12-21T12:12:07.291592","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-30T19:56:50.132659Z","iopub.execute_input":"2022-01-30T19:56:50.132952Z","iopub.status.idle":"2022-01-30T19:56:52.013814Z","shell.execute_reply.started":"2022-01-30T19:56:50.132916Z","shell.execute_reply":"2022-01-30T19:56:52.013078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\ntest_names, test_texts = [], []\nfor f in list(os.listdir('../input/feedback-prize-2021/test')):\n    test_names.append(f.replace('.txt', ''))\n    test_texts.append(open('../input/feedback-prize-2021/test/' + f, 'r').read())\ntest_texts = pd.DataFrame({'id': test_names, 'text': test_texts})\n\nSUBMISSION = False\nif len(test_names) > 5:\n      SUBMISSION = True\n\ntest_texts.head()","metadata":{"gradient":{"execution_count":9,"id":"00c14a04-6f87-416f-a18f-2f50165da64a","kernelId":""},"papermill":{"duration":0.083228,"end_time":"2021-12-21T12:12:09.396487","exception":false,"start_time":"2021-12-21T12:12:09.313259","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-30T19:56:52.016034Z","iopub.execute_input":"2022-01-30T19:56:52.01687Z","iopub.status.idle":"2022-01-30T19:56:52.047533Z","shell.execute_reply.started":"2022-01-30T19:56:52.016827Z","shell.execute_reply":"2022-01-30T19:56:52.046689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\ntest_names, train_texts = [], []\nfor f in tqdm(list(os.listdir('../input/feedback-prize-2021/train'))):\n    test_names.append(f.replace('.txt', ''))\n    train_texts.append(open('../input/feedback-prize-2021/train/' + f, 'r').read())\ntrain_text_df = pd.DataFrame({'id': test_names, 'text': train_texts})\ntrain_text_df.head()","metadata":{"gradient":{"execution_count":10,"id":"68a0dc11-1974-4acc-afe4-cf18979d2484","kernelId":""},"papermill":{"duration":38.695201,"end_time":"2021-12-21T12:12:48.120383","exception":false,"start_time":"2021-12-21T12:12:09.425182","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-30T19:56:52.048837Z","iopub.execute_input":"2022-01-30T19:56:52.049182Z","iopub.status.idle":"2022-01-30T19:57:27.774154Z","shell.execute_reply.started":"2022-01-30T19:56:52.049144Z","shell.execute_reply":"2022-01-30T19:57:27.772102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Convert Train Text to NER Labels\nWe will now convert all text words into NER labels and save in a dataframe.","metadata":{"papermill":{"duration":0.123678,"end_time":"2021-12-21T12:12:48.368476","exception":false,"start_time":"2021-12-21T12:12:48.244798","status":"completed"},"tags":[]}},{"cell_type":"code","source":"if not LOAD_TOKENS_FROM:\n    all_entities = []\n    for ii,i in enumerate(train_text_df.iterrows()):\n        if ii%100==0: print(ii,', ',end='')\n        total = i[1]['text'].split().__len__()\n        entities = [\"O\"]*total\n        for j in train_df[train_df['id'] == i[1]['id']].iterrows():\n            discourse = j[1]['discourse_type']\n            list_ix = [int(x) for x in j[1]['predictionstring'].split(' ')]\n            entities[list_ix[0]] = f\"B-{discourse}\"\n            for k in list_ix[1:]: entities[k] = f\"I-{discourse}\"\n        all_entities.append(entities)\n    train_text_df['entities'] = all_entities\n    train_text_df.to_csv('train_NER.csv',index=False)\n    \nelse:\n    from ast import literal_eval\n    train_text_df = pd.read_csv(f'{LOAD_TOKENS_FROM}/train_NER.csv')\n    # pandas saves lists as string, we must convert back\n    train_text_df.entities = train_text_df.entities.apply(lambda x: literal_eval(x) )\n    \nprint( train_text_df.shape )\ntrain_text_df.head()","metadata":{"gradient":{"execution_count":11,"id":"a6d20e74-8e25-4973-8953-f3b9acd81689","kernelId":""},"execution":{"iopub.status.busy":"2022-01-30T19:57:27.775544Z","iopub.execute_input":"2022-01-30T19:57:27.775801Z","iopub.status.idle":"2022-01-30T19:57:40.873198Z","shell.execute_reply.started":"2022-01-30T19:57:27.775768Z","shell.execute_reply":"2022-01-30T19:57:40.87246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CREATE DICTIONARIES THAT WE CAN USE DURING TRAIN AND INFER\noutput_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n\nlabels_to_ids = {v:k for k,v in enumerate(output_labels)}\nids_to_labels = {k:v for k,v in enumerate(output_labels)}\ndisc_type_to_ids = {'Evidence':(11,12),'Claim':(5,6),'Lead':(1,2),'Position':(3,4),'Counterclaim':(7,8),'Rebuttal':(9,10),'Concluding Statement':(13,14)}","metadata":{"gradient":{"execution_count":12,"id":"ee648d66-852f-47a5-a404-5890cde5c054","kernelId":""},"papermill":{"duration":0.940609,"end_time":"2021-12-21T12:18:50.456125","exception":false,"start_time":"2021-12-21T12:18:49.515516","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-30T19:57:40.874523Z","iopub.execute_input":"2022-01-30T19:57:40.875342Z","iopub.status.idle":"2022-01-30T19:57:40.882274Z","shell.execute_reply.started":"2022-01-30T19:57:40.875299Z","shell.execute_reply":"2022-01-30T19:57:40.881589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_to_ids","metadata":{"gradient":{"execution_count":13,"id":"181002ad-1436-4890-8230-149cd4e7fcc1","kernelId":""},"papermill":{"duration":0.994404,"end_time":"2021-12-21T12:18:52.798977","exception":false,"start_time":"2021-12-21T12:18:51.804573","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-30T19:57:40.883452Z","iopub.execute_input":"2022-01-30T19:57:40.884213Z","iopub.status.idle":"2022-01-30T19:57:40.892816Z","shell.execute_reply.started":"2022-01-30T19:57:40.884153Z","shell.execute_reply":"2022-01-30T19:57:40.891973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define the dataset function\nBelow is our PyTorch dataset function. It always outputs tokens and attention. During training it also provides labels. And during inference it also provides word ids to help convert token predictions into word predictions.\n\nNote that we use `text.split()` and `is_split_into_words=True` when we convert train text to labeled train tokens. This is how the HugglingFace tutorial does it. However, this removes characters like `\\n` new paragraph. If you want your model to see new paragraphs, then we need to map words to tokens ourselves using `return_offsets_mapping=True`. See my TensorFlow notebook [here][1] for an example.\n\nSome of the following code comes from the example at HuggingFace [here][2]. However I think the code at that link is wrong. The HuggingFace original code is [here][3]. With the flag `LABEL_ALL` we can either label just the first subword token (when one word has more than one subword token). Or we can label all the subword tokens (with the word's label). In this notebook version, we label all the tokens. There is a Kaggle discussion [here][4]\n\n[1]: https://www.kaggle.com/cdeotte/tensorflow-longformer-ner-cv-0-617\n[2]: https://huggingface.co/docs/transformers/custom_datasets#tok_ner\n[3]: https://github.com/huggingface/transformers/blob/86b40073e9aee6959c8c85fcba89e47b432c4f4d/examples/pytorch/token-classification/run_ner.py#L371\n[4]: https://www.kaggle.com/c/feedback-prize-2021/discussion/296713","metadata":{"papermill":{"duration":1.001889,"end_time":"2021-12-21T12:18:54.981896","exception":false,"start_time":"2021-12-21T12:18:53.980007","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Return an array that maps character index to index of word in list of split() words\ndef split_mapping(unsplit):\n    splt = unsplit.split()\n    offset_to_wordidx = np.full(len(unsplit),-1)\n    txt_ptr = 0\n    for split_index, full_word in enumerate(splt):\n        while unsplit[txt_ptr:txt_ptr + len(full_word)] != full_word:\n            txt_ptr += 1\n        offset_to_wordidx[txt_ptr:txt_ptr + len(full_word)] = split_index\n        txt_ptr += len(full_word)\n    return offset_to_wordidx","metadata":{"gradient":{"execution_count":14,"id":"24ebb3cc-c13f-4356-b3b4-6e9dba4727ff","kernelId":""},"execution":{"iopub.status.busy":"2022-01-30T19:57:40.894268Z","iopub.execute_input":"2022-01-30T19:57:40.894526Z","iopub.status.idle":"2022-01-30T19:57:40.902597Z","shell.execute_reply.started":"2022-01-30T19:57:40.894489Z","shell.execute_reply":"2022-01-30T19:57:40.901846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class dataset(Dataset):\n  def __init__(self, dataframe, tokenizer, max_len, get_wids):\n        self.len = len(dataframe)\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.get_wids = get_wids # for validation\n\n  def __getitem__(self, index):\n        # GET TEXT AND WORD LABELS \n        text = self.data.text[index]        \n        word_labels = self.data.entities[index] if not self.get_wids else None\n\n        # TOKENIZE TEXT\n        encoding = self.tokenizer(text,\n                             return_offsets_mapping=True, \n                             padding='max_length', \n                             truncation=True, \n                             max_length=self.max_len)\n        \n        word_ids = encoding.word_ids()  \n        split_word_ids = np.full(len(word_ids),-1)\n        offset_to_wordidx = split_mapping(text)\n        offsets = encoding['offset_mapping']\n        \n        # CREATE TARGETS AND MAPPING OF TOKENS TO SPLIT() WORDS\n        label_ids = []\n        # Iterate in reverse to label whitespace tokens until a Begin token is encountered\n        for token_idx, word_idx in reversed(list(enumerate(word_ids))):\n            \n            if word_idx is None:\n                if not self.get_wids: label_ids.append(-100)\n            else:\n                if offsets[token_idx] != (0,0):\n                    #Choose the split word that shares the most characters with the token if any\n                    split_idxs = offset_to_wordidx[offsets[token_idx][0]:offsets[token_idx][1]]\n                    split_index = stats.mode(split_idxs[split_idxs != -1]).mode[0] if len(np.unique(split_idxs)) > 1 else split_idxs[0]\n                    \n                    if split_index != -1: \n                        if not self.get_wids: label_ids.append( labels_to_ids[word_labels[split_index]] )\n                        split_word_ids[token_idx] = split_index\n                    else:\n                        # Even if we don't find a word, continue labeling 'I' tokens until a 'B' token is found\n                        if label_ids and label_ids[-1] != -100 and ids_to_labels[label_ids[-1]][0] == 'I':\n                            split_word_ids[token_idx] = split_word_ids[token_idx + 1]\n                            if not self.get_wids: label_ids.append(label_ids[-1])\n                        else:\n                            if not self.get_wids: label_ids.append(-100)\n                else:\n                    if not self.get_wids: label_ids.append(-100)\n        \n        encoding['labels'] = list(reversed(label_ids))\n\n        # CONVERT TO TORCH TENSORS\n        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n        if self.get_wids: \n            item['wids'] = torch.as_tensor(split_word_ids)\n        \n        return item\n\n  def __len__(self):\n        return self.len","metadata":{"gradient":{"execution_count":15,"id":"7c2ec5a9-c120-4883-934d-41a83e3da1cc","kernelId":""},"papermill":{"duration":0.934726,"end_time":"2021-12-21T12:18:56.852259","exception":false,"start_time":"2021-12-21T12:18:55.917533","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-30T19:57:40.906494Z","iopub.execute_input":"2022-01-30T19:57:40.906895Z","iopub.status.idle":"2022-01-30T19:57:40.921277Z","shell.execute_reply.started":"2022-01-30T19:57:40.906866Z","shell.execute_reply":"2022-01-30T19:57:40.920381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Train and Validation Dataloaders\nWe will use the same train and validation subsets as my TensorFlow notebook [here][1]. Then we can compare results. And/or experiment with ensembling the validation fold predictions.\n\n[1]: https://www.kaggle.com/cdeotte/tensorflow-longformer-ner-cv-0-617","metadata":{"papermill":{"duration":0.936225,"end_time":"2021-12-21T12:19:08.206923","exception":false,"start_time":"2021-12-21T12:19:07.270698","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# CHOOSE VALIDATION INDEXES (that match my TF notebook)\nIDS = train_df.id.unique()\n\n\nnp.random.seed(42)\n\nif FOLD == 'half':\n    train_idx = np.random.choice(np.arange(len(IDS)),int(0.5*len(IDS)),replace=False)\n    valid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\n    \nelif FOLD is not None:\n    print('There are',len(IDS),'train texts. We will split 93% 7% for ensemble training.')\n    shuffled_ids = np.arange(len(IDS))\n    np.random.shuffle(shuffled_ids)\n\n    valid_len = int(.07 * len(IDS))\n    valid_idx = shuffled_ids[FOLD*valid_len:(FOLD+1)*valid_len]\n    train_idx = np.setdiff1d(np.arange(len(IDS)),valid_idx)\n    \nelse:\n    print('There are',len(IDS),'train texts. We will split 90% 10% for ensemble training.')\n    train_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\n    valid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\n\nnp.random.seed(None)","metadata":{"gradient":{"execution_count":16,"id":"92015ad6-554b-4af0-b958-76b4def62ece","kernelId":""},"execution":{"iopub.status.busy":"2022-01-30T19:57:40.922495Z","iopub.execute_input":"2022-01-30T19:57:40.922986Z","iopub.status.idle":"2022-01-30T19:57:40.952538Z","shell.execute_reply.started":"2022-01-30T19:57:40.922952Z","shell.execute_reply":"2022-01-30T19:57:40.951825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CREATE TRAIN SUBSET AND VALID SUBSET\ndata = train_text_df[['id','text', 'entities']]\ntrain_dataset = data.loc[data['id'].isin(IDS[train_idx]),['text', 'entities']].reset_index(drop=True)\ntest_dataset = data.loc[data['id'].isin(IDS[valid_idx])].reset_index(drop=True)\n\nprint(\"FULL Dataset: {}\".format(data.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset.shape))\n\ntokenizer = AutoTokenizer.from_pretrained(DOWNLOADED_MODEL_PATH) \ntraining_set = dataset(train_dataset, tokenizer, config['max_length'], False)\ntesting_set = dataset(test_dataset, tokenizer, config['max_length'], True)","metadata":{"gradient":{"execution_count":17,"id":"ce4573a1-475a-45be-9bd8-bccef45cf5a6","kernelId":""},"papermill":{"duration":0.953973,"end_time":"2021-12-21T12:19:10.088215","exception":false,"start_time":"2021-12-21T12:19:09.134242","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-30T19:57:40.955382Z","iopub.execute_input":"2022-01-30T19:57:40.955779Z","iopub.status.idle":"2022-01-30T19:57:41.117643Z","shell.execute_reply.started":"2022-01-30T19:57:40.95574Z","shell.execute_reply":"2022-01-30T19:57:41.116931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAIN DATASET AND VALID DATASET\ntrain_params = {'batch_size': config['train_batch_size'],\n                'shuffle': True,\n                'num_workers': 2,\n                'pin_memory':True\n                }\n\ntest_params = {'batch_size': config['valid_batch_size'],\n                'shuffle': False,\n                'num_workers': 2,\n                'pin_memory':True\n                }\n\ntraining_loader = DataLoader(training_set, **train_params)\ntesting_loader = DataLoader(testing_set, **test_params)\n\n# TEST DATASET\ntest_texts_set = dataset(test_texts, tokenizer, config['max_length'], True)\ntest_texts_loader = DataLoader(test_texts_set, **test_params)","metadata":{"gradient":{"execution_count":18,"id":"3b670879-0d5d-44c6-950d-780518c2b3bf","kernelId":""},"papermill":{"duration":0.955464,"end_time":"2021-12-21T12:19:12.022567","exception":false,"start_time":"2021-12-21T12:19:11.067103","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-30T19:57:41.11884Z","iopub.execute_input":"2022-01-30T19:57:41.11909Z","iopub.status.idle":"2022-01-30T19:57:41.125367Z","shell.execute_reply.started":"2022-01-30T19:57:41.119053Z","shell.execute_reply":"2022-01-30T19:57:41.12462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Model\nThe PyTorch train function is taken from Raghavendrakotala's great notebook [here][1]. I assume it uses a masked loss which avoids computing loss when target is `-100`. If not, we need to update this.\n\n[1]: https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\ndef train(epoch):\n    tr_loss, tr_accuracy = 0, 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    #tr_preds, tr_labels = [], []\n    \n    # put model in training mode\n    model.train()\n    \n    for idx, batch in enumerate(training_loader):\n        \n        ids = batch['input_ids'].to(config['device'], dtype = torch.long)\n        mask = batch['attention_mask'].to(config['device'], dtype = torch.long)\n        labels = batch['labels'].to(config['device'], dtype = torch.long)\n\n        with amp.autocast():\n            loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels,\n                                   return_dict=False)\n        tr_loss += loss.item()\n\n        nb_tr_steps += 1\n        nb_tr_examples += labels.size(0)\n        \n        if idx % 200==0:\n            loss_step = tr_loss/nb_tr_steps\n            print(f\"Training loss after {idx:04d} training steps: {loss_step}\")\n           \n        # compute training accuracy\n        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n        \n        # only compute accuracy at active labels\n        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n        \n        labels = torch.masked_select(flattened_targets, active_accuracy)\n        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n        \n        #tr_labels.extend(labels)\n        #tr_preds.extend(predictions)\n\n        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n        tr_accuracy += tmp_tr_accuracy\n    \n        # gradient clipping\n        torch.nn.utils.clip_grad_norm_(\n            parameters=model.parameters(), max_norm=config['max_grad_norm']\n        )\n        \n        # backward pass\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n    epoch_loss = tr_loss / nb_tr_steps\n    tr_accuracy = tr_accuracy / nb_tr_steps\n    print(f\"Training loss epoch: {epoch_loss}\")\n    print(f\"Training accuracy epoch: {tr_accuracy}\")","metadata":{"gradient":{"execution_count":19,"id":"6527e498-bd3e-4e2b-90e9-ca99d795dd3a","kernelId":""},"papermill":{"duration":1.00345,"end_time":"2021-12-21T12:19:31.294225","exception":false,"start_time":"2021-12-21T12:19:30.290775","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-30T19:57:41.126899Z","iopub.execute_input":"2022-01-30T19:57:41.127404Z","iopub.status.idle":"2022-01-30T19:57:41.141338Z","shell.execute_reply.started":"2022-01-30T19:57:41.127369Z","shell.execute_reply":"2022-01-30T19:57:41.140622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CREATE MODEL\nscaler = amp.GradScaler()\nconfig_model = AutoConfig.from_pretrained(DOWNLOADED_MODEL_PATH+'/config.json') \nmodel = AutoModelForTokenClassification.from_pretrained(\n                   DOWNLOADED_MODEL_PATH+'/pytorch_model.bin',config=config_model)\nmodel.to(config['device'])\noptimizer = AdamW(params=model.parameters(), lr=config['learning_rates'][0])","metadata":{"gradient":{"execution_count":20,"id":"06cd88d7-637e-426c-94cc-78d0b8702cf3","kernelId":""},"execution":{"iopub.status.busy":"2022-01-30T19:57:41.142729Z","iopub.execute_input":"2022-01-30T19:57:41.14309Z","iopub.status.idle":"2022-01-30T19:57:52.803703Z","shell.execute_reply.started":"2022-01-30T19:57:41.143053Z","shell.execute_reply":"2022-01-30T19:57:52.802957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings('ignore', '.*__floordiv__ is deprecated.*',)\n\n# LOOP TO TRAIN MODEL (or load model)\nif not LOAD_MODEL_FROM:\n    for epoch in range(config['epochs']):\n        \n        print(f\"### Training epoch: {epoch + 1}\")\n        for g in optimizer.param_groups: \n            g['lr'] = config['learning_rates'][epoch]\n        lr = optimizer.param_groups[0]['lr']\n        print(f'### LR = {lr}\\n')\n        \n        train(epoch)\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n    torch.save(model.state_dict(), f'bigbird_v{VER}.pt')","metadata":{"gradient":{"execution_count":21,"id":"6c331f3a-7039-49ff-a97e-70ee5647c658","kernelId":""},"execution":{"iopub.status.busy":"2022-01-30T19:57:52.805083Z","iopub.execute_input":"2022-01-30T19:57:52.805365Z","iopub.status.idle":"2022-01-30T19:57:52.811502Z","shell.execute_reply.started":"2022-01-30T19:57:52.805328Z","shell.execute_reply":"2022-01-30T19:57:52.810855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference and Validation Code\nWe will infer in batches using our data loader which is faster than inferring one text at a time with a for-loop. The metric code is taken from Rob Mulla's great notebook [here][2]. Our model achieves validation F1 score 0.615! \n\nDuring inference our model will make predictions for each subword token. Some single words consist of multiple subword tokens. In the code below, we use a word's first subword token prediction as the label for the entire word. We can try other approaches, like averaging all subword predictions or taking `B` labels before `I` labels etc.\n\n[1]: https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\n[2]: https://www.kaggle.com/robikscube/student-writing-competition-twitch","metadata":{}},{"cell_type":"code","source":"\n# Returns per-word, mean class prediction probability over all tokens corresponding to each word\ndef inference(data_loader, model_ids):\n    \n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    ensemble_preds = np.zeros((len(data_loader.dataset), config['max_length'], len(labels_to_ids)), dtype=np.float32)\n    wids = np.full((len(data_loader.dataset), config['max_length']), -100)\n    for model_i, model_id in enumerate(model_ids):\n        \n        model.load_state_dict(torch.load(f'{LOAD_MODEL_FROM}/ensemble-{model_id}.pt', map_location=config['device']))\n        \n        # put model in training mode\n        model.eval()\n        for batch_i, batch in enumerate(data_loader):\n            \n            if model_i == 0: wids[batch_i*config['valid_batch_size']:(batch_i+1)*config['valid_batch_size']] = batch['wids'].numpy()\n\n            # MOVE BATCH TO GPU AND INFER\n            ids = batch[\"input_ids\"].to(config['device'])\n            mask = batch[\"attention_mask\"].to(config['device'])\n            with amp.autocast():\n                outputs = model(ids, attention_mask=mask, return_dict=False)\n            all_preds = torch.nn.functional.softmax(outputs[0], dim=2).cpu().detach().numpy() \n            ensemble_preds[batch_i*config['valid_batch_size']:(batch_i+1)*config['valid_batch_size']] += all_preds\n            \n            del ids\n            del mask\n            del outputs\n            del all_preds\n            \n        gc.collect()\n        torch.cuda.empty_cache()\n            \n    ensemble_preds /= len(model_ids)\n    predictions = []\n    # INTERATE THROUGH EACH TEXT AND GET PRED\n    for text_i in range(ensemble_preds.shape[0]):\n        token_preds = ensemble_preds[text_i]\n        \n        prediction = []\n        previous_word_idx = -1\n        prob_buffer = []\n        word_ids = wids[text_i][wids[text_i] != -100]\n        for idx,word_idx in enumerate(word_ids):                            \n            if word_idx == -1:\n                pass\n            elif word_idx != previous_word_idx:              \n                if prob_buffer:\n                    prediction.append(np.mean(prob_buffer, dtype=np.float32, axis=0))\n                    prob_buffer = []\n                prob_buffer.append(token_preds[idx])\n                previous_word_idx = word_idx\n            else: \n                prob_buffer.append(token_preds[idx])\n        prediction.append(np.mean(prob_buffer, dtype=np.float32, axis=0))\n        predictions.append(prediction)\n            \n    gc.collect()\n    torch.cuda.empty_cache()\n    return predictions","metadata":{"gradient":{"execution_count":22,"id":"93df9ccc-f7ff-4cb2-8e20-8030026fff42","kernelId":""},"execution":{"iopub.status.busy":"2022-01-30T19:57:52.813298Z","iopub.execute_input":"2022-01-30T19:57:52.81383Z","iopub.status.idle":"2022-01-30T19:57:52.830034Z","shell.execute_reply.started":"2022-01-30T19:57:52.813793Z","shell.execute_reply":"2022-01-30T19:57:52.829385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from Rob Mulla @robikscube\n# https://www.kaggle.com/robikscube/student-writing-competition-twitch\ndef calc_overlap(row):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(row.predictionstring_pred.split(' '))\n    set_gt = set(row.predictionstring_gt.split(' '))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter / len_gt\n    overlap_2 = inter/ len_pred\n    return [overlap_1, overlap_2]\n\n\ndef score_feedback_comp(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n        \n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df = pred_df[['id','class','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df.index\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(gt_df,\n                           left_on=['id','class'],\n                           right_on=['id','discourse_type'],\n                           how='outer',\n                           suffixes=('_pred','_gt')\n                          )\n    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n\n    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n\n\n    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n    tp_pred_ids = joined.query('potential_TP') \\\n        .sort_values('max_overlap', ascending=False) \\\n        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    #calc microf1\n    my_f1_score = TP / (TP + 0.5*(FP+FN))\n    return my_f1_score","metadata":{"gradient":{"execution_count":23,"id":"cadc897b-9143-4b68-960a-0b2d761b63cb","kernelId":""},"execution":{"iopub.status.busy":"2022-01-30T19:57:52.83273Z","iopub.execute_input":"2022-01-30T19:57:52.832916Z","iopub.status.idle":"2022-01-30T19:57:52.850087Z","shell.execute_reply.started":"2022-01-30T19:57:52.832893Z","shell.execute_reply":"2022-01-30T19:57:52.849352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Aggregate the per-word, mean class probability predictions from BigBird for validation and submit sets.","metadata":{}},{"cell_type":"code","source":"import pickle\nvalid = train_df.loc[train_df['id'].isin(IDS[valid_idx])]\n\nprint('Predicting with BigBird...')\nif not SUBMISSION:\n    try:\n        with open( KAGGLE_CACHE + \"/valid_preds.p\", \"rb\" ) as validFile:\n            valid_word_preds = pickle.load( validFile )    \n    except:\n        valid_word_preds = inference(testing_loader, ENSEMBLE_IDS)\nelse: valid_word_preds = []\n        \ntest_word_preds = inference(test_texts_loader, ENSEMBLE_IDS)\n    \nwith open( cache + \"/valid_preds.p\", \"wb\" ) as validFile:\n    pickle.dump( valid_word_preds, validFile )\nprint('Done.')\n\nuniqueValidGroups = range(len(valid_word_preds))\nuniqueSubmitGroups = range(len(test_word_preds))\n","metadata":{"gradient":{"execution_count":24,"id":"7597203a-32ac-477b-951e-befcb89c709a","kernelId":""},"execution":{"iopub.status.busy":"2022-01-30T19:57:52.851875Z","iopub.execute_input":"2022-01-30T19:57:52.852454Z","iopub.status.idle":"2022-01-30T19:58:32.943977Z","shell.execute_reply.started":"2022-01-30T19:57:52.852416Z","shell.execute_reply":"2022-01-30T19:58:32.943161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sequence Datasets\nWe will create datasets that, instead of describing individual words or tokens, describes sequences of words. Within some heuristic constraints, every possible sub-sequence of words in a text will converted to a dataset sample with the following attributes:\n\n* features- sequence length, position, and various kinds of class probability predictions/statistics\n* labels- whether the sequence matches exactly a discourse instance\n* truePos- whether the sequence matches a discourse instance by competition criteria for true positive\n* groups- the integer index of the text where the sequence is found\n* wordRanges- the start and end word index of the sequence in the text\n\nSequence datasets are generated for each discourse type and for validation and submission datasets. ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nfrom bisect import bisect_left\n\n# Percentile code taken from https://www.kaggle.com/vuxxxx/tensorflow-longformer-ner-postprocessing\n# Thank Vu!\n#\n# Use 99.5% of the distribution of lengths for a disourse type as maximum. \n# Increasing this constraint makes this step slower but generally increases performance.\nMAX_SEQ_LEN = {}\ntrain_df['len'] = train_df['predictionstring'].apply(lambda x:len(x.split()))\nmax_lens = train_df.groupby('discourse_type')['len'].quantile(.995)\nfor disc_type in disc_type_to_ids:\n    MAX_SEQ_LEN[disc_type] = int(max_lens[disc_type])\n\n#The minimum probability prediction for a 'B'egin class for which we will evaluate a word sequence\nMIN_BEGIN_PROB = {\n    'Claim': .35,\n    'Concluding Statement': .15,\n    'Counterclaim': .04,\n    'Evidence': .1,\n    'Lead': .32,\n    'Position': .25,\n    'Rebuttal': .01,\n}\n        \nclass SeqDataset(object):\n    \n    def __init__(self, features, labels, groups, wordRanges, truePos):\n        \n        self.features = np.array(features, dtype=np.float32)\n        self.labels = np.array(labels)\n        self.groups = np.array(groups, dtype=np.int16)\n        self.wordRanges = np.array(wordRanges, dtype=np.int16)\n        self.truePos = np.array(truePos)\n\n# Adapted from https://stackoverflow.com/questions/60467081/linear-interpolation-in-numpy-quantile\n# This is used to prevent re-sorting to compute quantile for every sequence.\ndef sorted_quantile(array, q):\n    array = np.array(array)\n    n = len(array)\n    index = (n - 1) * q\n    left = np.floor(index).astype(int)\n    fraction = index - left\n    right = left\n    right = right + (fraction > 0).astype(int)\n    i, j = array[left], array[right]\n    return i + (j - i) * fraction\n        \ndef seq_dataset(disc_type, pred_indices=None, submit=False):\n    word_preds = valid_word_preds if not submit else test_word_preds\n    window = pred_indices if pred_indices else range(len(word_preds))\n    X = np.empty((int(1e6),13), dtype=np.float32)\n    X_ind = 0\n    y = []\n    truePos = []\n    wordRanges = []\n    groups = []\n    for text_i in tqdm(window):\n        text_preds = np.array(word_preds[text_i])\n        num_words = len(text_preds)\n        disc_begin, disc_inside = disc_type_to_ids[disc_type]\n        \n        # The probability that a word corresponds to either a 'B'-egin or 'I'-nside token for a class\n        prob_or = lambda word_preds: (1-(1-word_preds[:,disc_begin]) * (1-word_preds[:,disc_inside]))\n        \n        if not submit:\n            gt_idx = set()\n            gt_arr = np.zeros(num_words, dtype=int)\n            text_gt = valid.loc[valid.id == test_dataset.id.values[text_i]]\n            disc_gt = text_gt.loc[text_gt.discourse_type == disc_type]\n            \n            # Represent the discourse instance locations in a hash set and an integer array for speed\n            for row_i, row in enumerate(disc_gt.iterrows()):\n                splt = row[1]['predictionstring'].split()\n                start, end = int(splt[0]), int(splt[-1]) + 1\n                gt_idx.add((start, end))\n                gt_arr[start:end] = row_i + 1\n            gt_lens = np.bincount(gt_arr)\n        \n        # Iterate over every sub-sequence in the text\n        quants = np.linspace(0,1,7)\n        prob_begins = np.copy(text_preds[:,disc_begin])\n        min_begin = MIN_BEGIN_PROB[disc_type]\n        for pred_start in range(num_words):\n            prob_begin = prob_begins[pred_start]\n            if prob_begin > min_begin:\n                begin_or_inside = []\n                for pred_end in range(pred_start+1,min(num_words+1, pred_start+MAX_SEQ_LEN[disc_type]+1)):\n                    \n                    new_prob = prob_or(text_preds[pred_end-1:pred_end])\n                    insert_i = bisect_left(begin_or_inside, new_prob)\n                    begin_or_inside.insert(insert_i, new_prob[0])\n\n                    # Generate features for a word sub-sequence\n\n                    # The length and position of start/end of the sequence\n                    features = [pred_end - pred_start, pred_start / float(num_words), pred_end / float(num_words)]\n                    \n                    # 7 evenly spaced quantiles of the distribution of relevant class probabilities for this sequence\n                    features.extend(list(sorted_quantile(begin_or_inside, quants)))\n\n                    # The probability that words on either edge of the current sub-sequence belong to the class of interest\n                    features.append(prob_or(text_preds[pred_start-1:pred_start])[0] if pred_start > 0 else 0)\n                    features.append(prob_or(text_preds[pred_end:pred_end+1])[0] if pred_end < num_words else 0)\n\n                    # The probability that the first word corresponds to a 'B'-egin token\n                    features.append(text_preds[pred_start,disc_begin])\n\n                    exact_match = (pred_start, pred_end) in gt_idx if not submit else None\n\n                    if not submit:\n                        true_pos = False\n                        for match_cand, count in Counter(gt_arr[pred_start:pred_end]).most_common(2):\n                            if match_cand != 0 and count / float(pred_end - pred_start) >= .5 and float(count) / gt_lens[match_cand] >= .5: true_pos = True\n                    else: true_pos = None\n\n                    # For efficiency, use a numpy array instead of a list that doubles in size when full to conserve constant \"append\" time complexity\n                    if X_ind >= X.shape[0]:\n                        new_X = np.empty((X.shape[0]*2,13), dtype=np.float32)\n                        new_X[:X.shape[0]] = X\n                        X = new_X\n                    X[X_ind] = features\n                    X_ind += 1\n                    \n                    y.append(exact_match)\n                    truePos.append(true_pos)\n                    wordRanges.append((np.int16(pred_start), np.int16(pred_end)))\n                    groups.append(np.int16(text_i))\n\n    return SeqDataset(X[:X_ind], y, groups, wordRanges, truePos)\n","metadata":{"gradient":{"execution_count":25,"id":"268caa30-cdb4-4dbe-873e-60d40637e003","kernelId":""},"execution":{"iopub.status.busy":"2022-01-30T19:58:32.945886Z","iopub.execute_input":"2022-01-30T19:58:32.946179Z","iopub.status.idle":"2022-01-30T19:58:33.330765Z","shell.execute_reply.started":"2022-01-30T19:58:32.946141Z","shell.execute_reply":"2022-01-30T19:58:33.330069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from joblib import Parallel, delayed\nfrom multiprocessing import Manager\n\nmanager = Manager()\n\n\ndef sequenceDataset(disc_type, submit=False):\n    if not submit: validSeqSets[disc_type] = seq_dataset(disc_type) if not SUBMISSION else None\n    else: submitSeqSets[disc_type] = seq_dataset(disc_type, submit=True)\n\ntry:\n    with open( KAGGLE_CACHE + \"/valid_seqds.p\", \"rb\" ) as validFile:\n        validSeqSets = pickle.load( validFile )  \nexcept:\n    print('Making validation sequence datasets...')\n    validSeqSets = manager.dict()\n    Parallel(n_jobs=-1, backend='multiprocessing')(\n            delayed(sequenceDataset)(disc_type, False) \n           for disc_type in disc_type_to_ids\n        )\n    print('Done.')\n    \n    \nprint('Making submit sequence datasets...')\nsubmitSeqSets = manager.dict()\nParallel(n_jobs=-1, backend='multiprocessing')(\n        delayed(sequenceDataset)(disc_type, True) \n       for disc_type in disc_type_to_ids\n    )\nprint('Done.')\n    \nwith open( cache + \"/valid_seqds.p\", \"wb\" ) as validFile:\n    pickle.dump( dict(validSeqSets), validFile )","metadata":{"gradient":{"execution_count":26,"id":"a03de634-8afe-4e9f-8634-2c1e932aa64c","kernelId":""},"execution":{"iopub.status.busy":"2022-01-30T19:58:33.331978Z","iopub.execute_input":"2022-01-30T19:58:33.332402Z","iopub.status.idle":"2022-01-30T19:58:36.829458Z","shell.execute_reply.started":"2022-01-30T19:58:33.332364Z","shell.execute_reply":"2022-01-30T19:58:36.828596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NEGATIVE_SAMPLE_RATIO = 1\n\n# Downsample negative samples to 1:1 for efficiency/ease. There are many samples, and performance increase was observed.\ndef resample(y):\n    global resample_call\n    counts = np.bincount(y)\n    np.random.seed((resample_call+counts[0]) % 2**32)\n    \n    neg_sample_count = NEGATIVE_SAMPLE_RATIO*counts[1]\n    indices = np.concatenate((\n        np.random.choice(np.arange(len(y))[y==0], neg_sample_count, replace=False),\n        np.arange(len(y))[y==1]\n    ))\n    indices.sort()\n    resample_call += 1\n    return indices\n\nresample_call = 0","metadata":{"execution":{"iopub.status.busy":"2022-01-30T19:58:36.832692Z","iopub.execute_input":"2022-01-30T19:58:36.832929Z","iopub.status.idle":"2022-01-30T19:58:36.840386Z","shell.execute_reply.started":"2022-01-30T19:58:36.832899Z","shell.execute_reply":"2022-01-30T19:58:36.83958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sequence Classifier Tuning\n\nEvery discorse type/class is trained in separate, parallel optimization loops (if you have enough CPUs).\n\nDuring a training iteration, the validation set is split into 8 folds. For every fold, a classifier is trained on 7 folds to predict the probability that a word sub-sequence is a true positive in the remaining fold. To compose a set of predictions for a text, those sub-sequences with the highest predicted probability of being a true positive are included iteratively, so long as they do not intersect with previously included sub-sequences. Sub-sequences are no longer included when their predicted probability is beneath a threshold. Tuing this per-class theshold is the objective of this tuning stage.\n\nOnce a set of predicted sub-sequences is composed for each text in each fold, the competition Macro F1 score is computed for the entire validation set. A noisy optimization algorithm, `skopt.gp_minimize`, is used to find the optimal probability threshold described above for each class with minimal iterations. After tuning is complete, the classifier in each fold is saved to disc. The mean probability predictions of this ensemble of classifiers is used during submission.","metadata":{}},{"cell_type":"code","source":"from joblib import Parallel, delayed\nfrom multiprocessing import Manager\nfrom sklearn.model_selection import cross_val_score, GroupKFold\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom skopt.space import Real\nfrom skopt import gp_minimize\nimport sys\nimport xgboost\n\nNUM_FOLDS = 8\n\nwarnings.filterwarnings('ignore', '.*ragged nested sequences*',)\n\nseq_cache = {} # For each fold and each text. cache score predictions sorted by score\nclfs = []  # Each fold will add its classifier here\n# Predict sub-sequences for a discourse type and set of train/test texts\ndef predict_strings(disc_type, probThresh, test_groups, train_ind=None, submit=False):\n    string_preds = []\n    validSeqDs = validSeqSets[disc_type]\n    submitSeqDs = submitSeqSets[disc_type]\n    \n    # Average the probability predictions of a set of classifiers\n    get_tp_prob = lambda testDs, classifiers: np.mean([clf.predict_proba(testDs.features)[:,1] for clf in classifiers], axis=0) if testDs.features.shape[0] > 0 else np.array([])\n    \n    if not submit:\n        # Point to validation set values\n        predict_df = test_dataset\n        text_df = train_text_df\n        groupIdx = np.isin(validSeqDs.groups, test_groups)\n        testDs = SeqDataset(validSeqDs.features[groupIdx], validSeqDs.labels[groupIdx], validSeqDs.groups[groupIdx], validSeqDs.wordRanges[groupIdx], validSeqDs.truePos[groupIdx])\n        \n        # Cache the classifier predictions to speed up tuning iterations\n        seq_key = (disc_type, tuple(test_groups), tuple(train_ind))\n        if seq_key in seq_cache:\n            text_to_seq = seq_cache[seq_key]\n        else:\n\n            clf = xgboost.XGBClassifier(\n                learning_rate = 0.05,\n                n_estimators=200,\n                max_depth=7,\n                min_child_weight=5,\n                gamma=0,\n                subsample=0.7,\n                reg_alpha=.0005,\n                colsample_bytree=0.6,\n                scale_pos_weight=1,\n                use_label_encoder=False,\n                eval_metric='logloss',\n                tree_method='hist'\n            )\n            \n            resampled = resample(validSeqDs.truePos[train_ind])\n            clf.fit(validSeqDs.features[train_ind][resampled],validSeqDs.truePos[train_ind][resampled])\n            clfs.append(clf)\n            prob_tp = get_tp_prob(testDs, [clf])\n        \n    else:\n        # Point to submission set values\n        predict_df = test_texts\n        text_df = test_texts\n        groupIdx = np.isin(submitSeqDs.groups, test_groups)\n        testDs = SeqDataset(submitSeqDs.features[groupIdx], submitSeqDs.labels[groupIdx], submitSeqDs.groups[groupIdx], submitSeqDs.wordRanges[groupIdx], submitSeqDs.truePos[groupIdx])\n        \n        # Classifiers are always loaded from disc during submission\n        with open( f\"../input/seqclassifiers6/{disc_type}_clf.p\", \"rb\" ) as clfFile:\n            classifiers = pickle.load( clfFile )  \n        prob_tp = get_tp_prob(testDs, classifiers)\n        \n    if submit or seq_key not in seq_cache:\n        text_to_seq = {}\n        for text_idx in test_groups:\n            # The probability of true positive and (start,end) of each sub-sequence in the curent text\n            prob_tp_curr = prob_tp[testDs.groups == text_idx]\n            word_ranges_curr = testDs.wordRanges[testDs.groups == text_idx]\n            sorted_seqs = list(reversed(sorted(zip(prob_tp_curr, [tuple(wr) for wr in word_ranges_curr]))))\n            text_to_seq[text_idx] = sorted_seqs\n        if not submit: seq_cache[seq_key] = text_to_seq\n    \n    for text_idx in test_groups:\n        \n        i = 1\n        split_text = text_df.loc[text_df.id == predict_df.id.values[text_idx]].iloc[0].text.split()\n        \n        # Start and end word indices of sequence candidates kept in sorted order for efficiency\n        starts = []\n        ends = []\n        \n        # Include the sub-sequence predictions in order of predicted probability\n        for prob, wordRange in text_to_seq[text_idx]:\n            \n            # Until the predicted probability is lower than the tuned threshold\n            if prob < probThresh: break\n                \n            # Binary search already-placed word sequence intervals, and insert the new word sequence interval if it does not intersect an existing interval.\n            insert = bisect_left(starts, wordRange[0])\n            if (insert == 0 or ends[insert-1] <= wordRange[0]) and (insert == len(starts) or starts[insert] >= wordRange[1]):\n                starts.insert(insert, wordRange[0])\n                ends.insert(insert, wordRange[1])\n                string_preds.append((predict_df.id.values[text_idx], disc_type, ' '.join(map(str, list(range(wordRange[0], wordRange[1]))))))\n                i += 1     \n    return string_preds\n\ndef sub_df(string_preds):\n    return pd.DataFrame(string_preds, columns=['id','class','predictionstring'])\n    \n# Convert skopt's uniform distribution over the tuning threshold to a distribution that exponentially decays from 100% to 0%\ndef prob_thresh(x): \n    return .01*(100-np.exp(100*x))\n\n# Convert back to the scalar supplied by skopt\ndef skopt_thresh(x): \n    return np.log((x/.01-100.)/-1.)/100.\n    \n# This function is called every tuning iteration.\n# It takes the probability threshold as input and returns Macro F1\ndef score_fmin(arr, disc_type):\n    validSeqDs = validSeqSets[disc_type]\n    string_preds = []\n    folds = np.array(list(GroupKFold(n_splits=NUM_FOLDS).split(validSeqDs.features, groups=validSeqDs.groups)))\n    gt_indices = []\n    for ind in folds[:,1]: gt_indices.extend(ind)\n        \n    # Texts that have no samples in our dataset for this class\n    unsampled_texts = np.array(np.array_split(list(set(uniqueValidGroups).difference(set(np.unique(validSeqDs.groups)))), NUM_FOLDS))\n    \n    gt_texts = test_dataset.id.values[np.unique(validSeqDs.groups[np.array(gt_indices, dtype=int)]).astype(int)]\n    \n    # Generate predictions from each fold of the validation set\n    for fold_i, (train_ind, test_ind) in enumerate(folds):\n        string_preds.extend(predict_strings(disc_type, prob_thresh(arr[0]), np.concatenate((np.unique(validSeqDs.groups[test_ind]), unsampled_texts[fold_i])).astype(int), train_ind))\n    boost_df = sub_df(list(string_preds))\n    gt_df = valid.loc[np.bitwise_and(valid['discourse_type']==disc_type, valid.id.isin(gt_texts))].copy()\n    f1 = score_feedback_comp(boost_df.copy(), gt_df)\n    return -f1\n\n\ndef train_seq_clfs(disc_type):\n    # The optimization bounds on the tuned probability threshold \n    space_start = skopt_thresh(.999)\n    space_end = skopt_thresh(0)\n    space  = [Real(space_start,space_end)]\n    \n    # Minimize F1\n    score_fmin_disc = lambda arr: score_fmin(arr, disc_type)\n    res_gp = gp_minimize(score_fmin_disc, space, n_calls=100, x0=[skopt_thresh(.5)])\n    \n    # Use the gaussian approximation of f(threshold) -> F1 to select the minima\n    thresh_cand = np.rot90([np.linspace(0,1,1000)])\n    cand_scores = res_gp.models[-1].predict(thresh_cand)\n    best_thresh_raw = space_start + (space_end - space_start)*thresh_cand[np.argmin(cand_scores)][0]\n    best_thresh = prob_thresh(best_thresh_raw)\n    exp_score = -np.min(cand_scores)\n    \n    # Make predictions at the inferred function minimum\n    pred_thresh_score = -score_fmin_disc([best_thresh_raw])\n    \n    # And the best iteration in the optimization run\n    best_iter_score = -score_fmin_disc(res_gp.x)\n    \n    # Save the trained classifiers to disc\n    with open( f\"{disc_type}_clf.p\", \"wb\" ) as clfFile:\n        pickle.dump( clfs, clfFile )\n        \n    # Save the tuning run results to file\n    with open( f\"{disc_type}_res.p\", \"wb\" ) as resFile:\n        pickle.dump( \n            {\n                'pred_thresh': best_thresh,  # The location of the minimum of the gaussian function inferred by skopt\n                'min_thresh': prob_thresh(res_gp.x[0]),  # The threshold which produces the best score\n                'pred_score': exp_score,  # The minimum of the gaussian function inferred by skopt\n                'min_score': best_iter_score, # The best score in the tuning run\n                'pred_thresh_score': pred_thresh_score  # The score produced by 'pred_thresh'\n            }, \n            resFile \n        )\n    print('Done training', disc_type)\n    \nif TRAIN_SEQ_CLASSIFIERS and not SUBMISSION:\n    print('Training sequence classifiers... (This takes a long time.)')\n    Parallel(n_jobs=-1, backend='multiprocessing')(\n            delayed(train_seq_clfs)(disc_type) \n           for disc_type in disc_type_to_ids\n    )\n    print('Done training all sequence classifiers.')","metadata":{"gradient":{"execution_count":27,"id":"cc2e7c81-2347-4b3d-90c6-2b52d2ffa036","kernelId":""},"execution":{"iopub.status.busy":"2022-01-30T19:58:36.841788Z","iopub.execute_input":"2022-01-30T19:58:36.842179Z","iopub.status.idle":"2022-01-30T19:58:37.155677Z","shell.execute_reply.started":"2022-01-30T19:58:36.842143Z","shell.execute_reply":"2022-01-30T19:58:37.154953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load the tuned probability thresholds from tuning result files, and make sub-sequence predictions!","metadata":{}},{"cell_type":"code","source":"thresholds = {}\nfor disc_type in disc_type_to_ids:\n    with open( f\"../input/seqclassifiers6/{disc_type}_res.p\", \"rb\" ) as res_file:\n        train_result = pickle.load( res_file )  \n    thresholds[disc_type] = train_result['pred_thresh']\n    print(disc_type, train_result)\nsub = pd.concat([sub_df(predict_strings(disc_type, thresholds[disc_type], uniqueSubmitGroups, submit=True)) for disc_type in disc_type_to_ids ]).reset_index(drop=True)","metadata":{"gradient":{"execution_count":28,"id":"087e600e-f15f-4154-8c26-6a8dabcfb73b","kernelId":""},"execution":{"iopub.status.busy":"2022-01-30T19:58:37.15684Z","iopub.execute_input":"2022-01-30T19:58:37.157087Z","iopub.status.idle":"2022-01-30T19:58:38.166Z","shell.execute_reply.started":"2022-01-30T19:58:37.157054Z","shell.execute_reply":"2022-01-30T19:58:38.164909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)","metadata":{"gradient":{"execution_count":29,"id":"658e2ab3-b02d-449c-ad21-340e6206586f","kernelId":""},"papermill":{"duration":1.020359,"end_time":"2021-12-21T12:58:54.413788","exception":false,"start_time":"2021-12-21T12:58:53.393429","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-30T19:58:38.166993Z","iopub.status.idle":"2022-01-30T19:58:38.167908Z","shell.execute_reply.started":"2022-01-30T19:58:38.167653Z","shell.execute_reply":"2022-01-30T19:58:38.167681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(sub)","metadata":{"gradient":{"execution_count":30,"id":"b2bd03cf-0460-446d-9340-71b477c78193","kernelId":""},"execution":{"iopub.status.busy":"2022-01-30T19:58:38.169209Z","iopub.status.idle":"2022-01-30T19:58:38.169831Z","shell.execute_reply.started":"2022-01-30T19:58:38.16958Z","shell.execute_reply":"2022-01-30T19:58:38.169606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"gradient":{"id":"0deaf0ca-410b-4e2f-882b-617d1c262662","kernelId":""}},"execution_count":null,"outputs":[]}]}