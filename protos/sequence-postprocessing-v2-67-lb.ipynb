{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "da4ac754-386b-43f7-b04a-b5f36cdca216",
     "kernelId": ""
    }
   },
   "source": [
    "# Sequence Post-Processing V2\n",
    "\n",
    "This is a fork of Chris Deotte's [notebook](https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615). Thanks Chris!\n",
    "\n",
    "This notebook contains an ensemble of 5 BigBird models trained on different subsets of the dataset. It also contains a post-processing pipeline that uses XGBoost to convert token-level predictions into sequence-level predictions. Because models like BigBird use a per-token loss function, and this competition uses a per-sequence scoring metric, post-processing token or word predictions into sequence predictions can yield signficant performance gains. Each model in the ensemble is trained on a randomly chosen 93% subset of the entire train set, with a distinct 7% subset left out from each training set.\n",
    "\n",
    "Additionally, this notebook shows how this competition can be treated as 7 separate classification tasks. This is because, to my knowledge, the scoring metric does not take discourse type/class intersections into account. See this [discussion](https://www.kaggle.com/c/feedback-prize-2021/discussion/300001). Each class is predicted with a separate sequence classifier, and no measures are taken to prevent sequence intersections between classes.\n",
    "\n",
    "Secondary datasets are created for which each sample is a sub-sequence of words in a text. Class probability predictions from BigBird are used to generate features for each of these samples. A gradient boosting classifier is trained to predict the probability of a true positive for each discourse type. It was discovered that training these classifiers on out-of-sample predictions did not yield a significant performance increase, so they were trained on as much of the dataset as practically possible (50% at present). \n",
    "\n",
    "Most of the post-processing code and description is at the bottom of this notebook.\n",
    "\n",
    "Currently this notebook uses\n",
    "\n",
    "* backbone BigBird  (with HuggingFace's head for TokenClassification)\n",
    "* NER formulation (with `is_split_into_words=False` tokenization)\n",
    "* 5 models trained on 93% of training data\n",
    "\n",
    "This notebook uses many code cells from Raghavendrakotala's great notebook [here][1]. Don't forget to upvote Raghavendrakotala's notebook :-)\n",
    "\n",
    "[1]: https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\n",
    "[2]: https://www.kaggle.com/cdeotte/tensorflow-longformer-ner-cv-0-617\n",
    "[3]: https://arxiv.org/abs/2007.14062\n",
    "\n",
    "**Changes in V2:**\n",
    "* BigBird ensemble\n",
    "* larger sequence classifier training set\n",
    "* tuned XGBoost model\n",
    "* sequence position features\n",
    "* more inclusive heuristic constraints for considered word sub-sequences\n",
    "* resampling samples in sequence classification to 1:1\n",
    "* AdamW net training\n",
    "* more granular features (7 quantiles instead of 5) to represent distribution of class probabilites for a sequence\n",
    "* increased iterations (100) of sequence classifier probability threshold tuning\n",
    "* computation efficiency improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Discussion: Ideas for Future Work](https://www.kaggle.com/c/feedback-prize-2021/discussion/308511)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "e0167440-e64f-45dd-89fd-961bba1bea34",
     "kernelId": ""
    }
   },
   "source": [
    "# Configuration\n",
    "This notebook can either train a new model or load a previously trained model (made from previous notebook version). Furthermore, this notebook can either create new NER labels or load existing NER labels (made from previous notebook version). In this notebook version, we will load model and load NER labels.\n",
    "\n",
    "Also this notebook can load huggingface stuff (like tokenizers) from a Kaggle dataset, or download it from internet. (If it downloads from internet, you can then put it in a Kaggle dataset, so next time you can turn internet off)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 2,
     "id": "8e783988-827d-4a32-9402-61603b89347b",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# DECLARE HOW MANY GPUS YOU WISH TO USE. \n",
    "# KAGGLE ONLY HAS 1, BUT OFFLINE, YOU CAN USE MORE\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #0,1,2,3 for four gpu\n",
    "\n",
    "# VERSION FOR SAVING MODEL WEIGHTS\n",
    "VER=26\n",
    "\n",
    "# IF VARIABLE IS NONE, THEN NOTEBOOK COMPUTES TOKENS\n",
    "# OTHERWISE NOTEBOOK LOADS TOKENS FROM PATH\n",
    "LOAD_TOKENS_FROM = '../input/py-bigbird-v26'\n",
    "\n",
    "# IF VARIABLE IS NONE, THEN NOTEBOOK TRAINS A NEW MODEL\n",
    "# OTHERWISE IT LOADS YOUR PREVIOUSLY TRAINED MODEL\n",
    "LOAD_MODEL_FROM = '../input/fullensemble'\n",
    "\n",
    "# Use the entire ensemble.\n",
    "ENSEMBLE_IDS = list(range(5))\n",
    "\n",
    "# Setting Fold = None leaves out an arbitrary 10% of the dataset for sequence classifier training.\n",
    "# Setting Fold to one of [0,1,2,3,4] leaves out the portion of the dataset not trained on by the corresponding ensemble model.\n",
    "# 'half' leaves out an arbitrary 50%.\n",
    "FOLD = None\n",
    "\n",
    "# IF FOLLOWING IS NONE, THEN NOTEBOOK \n",
    "# USES INTERNET AND DOWNLOADS HUGGINGFACE \n",
    "# CONFIG, TOKENIZER, AND MODEL\n",
    "DOWNLOADED_MODEL_PATH = '../input/py-bigbird-v26' \n",
    "\n",
    "if DOWNLOADED_MODEL_PATH is None:\n",
    "    DOWNLOADED_MODEL_PATH = 'model'    \n",
    "MODEL_NAME = 'google/bigbird-roberta-base'\n",
    "\n",
    "# Tune the probability threshold for sequence classifiers to maximize F1\n",
    "TRAIN_SEQ_CLASSIFIERS = False\n",
    "\n",
    "# A cache of the BigBird predictions for the validation/sequence training set and the corresponding sequence dataset\n",
    "KAGGLE_CACHE = '../input/feedbackcache2'\n",
    "\n",
    "cache = 'cache'\n",
    "cacheExists = os.path.exists(cache)\n",
    "if not cacheExists:\n",
    "  os.makedirs(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 3,
     "id": "568dee32-38d0-4852-af82-c39e404c588a",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "# skopt optimizer has a bug when scipy is installed with its default version\n",
    "if TRAIN_SEQ_CLASSIFIERS:\n",
    "    os.system('pip install --no-dependencies scipy==1.5.2 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 4,
     "id": "42571122-fde4-4d21-959f-2bd6327e8741",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "config = {'model_name': MODEL_NAME,   \n",
    "         'max_length': 1024,\n",
    "         'train_batch_size':4,\n",
    "         'valid_batch_size':4,\n",
    "         'epochs':5,\n",
    "         'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n",
    "         'max_grad_norm':10,\n",
    "         'device': 'cuda' if cuda.is_available() else 'cpu'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "a6cdc617-5a00-4369-8c04-a2f423e2fe26",
     "kernelId": ""
    }
   },
   "source": [
    "# How To Submit PyTorch Without Internet\n",
    "Many people ask me, how do I submit PyTorch models without internet? With HuggingFace Transformer, it's easy. Just download the following 3 things (1) model weights, (2) tokenizer files, (3) config file, and upload them to a Kaggle dataset. Below shows code how to get the files from HuggingFace for Google's BigBird-base. But this same code can download any transformer, like for example roberta-base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 5,
     "id": "966f07cc-d67a-468c-9f3a-c65f8608309e",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "if DOWNLOADED_MODEL_PATH == 'model':\n",
    "    os.mkdir('model')\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)\n",
    "    tokenizer.save_pretrained('model')\n",
    "\n",
    "    config_model = AutoConfig.from_pretrained(MODEL_NAME) \n",
    "    config_model.num_labels = 15\n",
    "    config_model.save_pretrained('model')\n",
    "\n",
    "    backbone = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, \n",
    "                                                               config=config_model)\n",
    "    backbone.save_pretrained('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "dabd82c1-80df-4783-a17e-f4728b2fcf27",
     "kernelId": ""
    }
   },
   "source": [
    "# Load Data and Libraries\n",
    "In addition to loading the train dataframe, we will load all the train and text files and save them in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "gradient": {
     "editing": false,
     "execution_count": 6,
     "id": "685d622c-38b1-4e57-99ff-c19b4d0e4b63",
     "kernelId": ""
    },
    "papermill": {
     "duration": 3.432226,
     "end_time": "2021-12-21T12:12:07.26275",
     "exception": false,
     "start_time": "2021-12-21T12:12:03.830524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np, os \n",
    "from scipy import stats\n",
    "import pandas as pd, gc \n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AdamW\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.cuda import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gradient": {
     "execution_count": 8,
     "id": "c48d29c7-0d7d-4782-ac7d-4f32e7586bac",
     "kernelId": ""
    },
    "papermill": {
     "duration": 1.866495,
     "end_time": "2021-12-21T12:12:09.158087",
     "exception": false,
     "start_time": "2021-12-21T12:12:07.291592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144293, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>230.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>They are some really bad consequences when stu...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>313.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>Some certain areas in the United States ban ph...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 1</td>\n",
       "      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>402.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>When people have phones, they know about certa...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 2</td>\n",
       "      <td>76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>759.0</td>\n",
       "      <td>886.0</td>\n",
       "      <td>Driving is one of the way how to get around. P...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 1</td>\n",
       "      <td>139 140 141 142 143 144 145 146 147 148 149 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  discourse_id  discourse_start  discourse_end  \\\n",
       "0  423A1CA112E2  1.622628e+12              8.0          229.0   \n",
       "1  423A1CA112E2  1.622628e+12            230.0          312.0   \n",
       "2  423A1CA112E2  1.622628e+12            313.0          401.0   \n",
       "3  423A1CA112E2  1.622628e+12            402.0          758.0   \n",
       "4  423A1CA112E2  1.622628e+12            759.0          886.0   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Modern humans today are always on their phone....           Lead   \n",
       "1  They are some really bad consequences when stu...       Position   \n",
       "2  Some certain areas in the United States ban ph...       Evidence   \n",
       "3  When people have phones, they know about certa...       Evidence   \n",
       "4  Driving is one of the way how to get around. P...          Claim   \n",
       "\n",
       "  discourse_type_num                                   predictionstring  \n",
       "0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n",
       "1         Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  \n",
       "2         Evidence 1    60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  \n",
       "3         Evidence 2  76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...  \n",
       "4            Claim 1  139 140 141 142 143 144 145 146 147 148 149 15...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../input/feedback-prize-2021/train.csv')\n",
    "print( train_df.shape )\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gradient": {
     "execution_count": 9,
     "id": "00c14a04-6f87-416f-a18f-2f50165da64a",
     "kernelId": ""
    },
    "papermill": {
     "duration": 0.083228,
     "end_time": "2021-12-21T12:12:09.396487",
     "exception": false,
     "start_time": "2021-12-21T12:12:09.313259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>During a group project, have you ever asked a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>80% of Americans believe seeking multiple opin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D46BCB48440A</td>\n",
       "      <td>When people ask for advice,they sometimes talk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>Making choices in life can be very difficult. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DF920E0A7337</td>\n",
       "      <td>Have you ever asked more than one person for h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text\n",
       "0  0FB0700DAF44  During a group project, have you ever asked a ...\n",
       "1  18409261F5C2  80% of Americans believe seeking multiple opin...\n",
       "2  D46BCB48440A  When people ask for advice,they sometimes talk...\n",
       "3  D72CB1C11673  Making choices in life can be very difficult. ...\n",
       "4  DF920E0A7337  Have you ever asked more than one person for h..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\n",
    "test_names, test_texts = [], []\n",
    "for f in list(os.listdir('../input/feedback-prize-2021/test')):\n",
    "    test_names.append(f.replace('.txt', ''))\n",
    "    test_texts.append(open('../input/feedback-prize-2021/test/' + f, 'r').read())\n",
    "test_texts = pd.DataFrame({'id': test_names, 'text': test_texts})\n",
    "\n",
    "SUBMISSION = False\n",
    "if len(test_names) > 5:\n",
    "      SUBMISSION = True\n",
    "\n",
    "test_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "gradient": {
     "execution_count": 10,
     "id": "68a0dc11-1974-4acc-afe4-cf18979d2484",
     "kernelId": ""
    },
    "papermill": {
     "duration": 38.695201,
     "end_time": "2021-12-21T12:12:48.120383",
     "exception": false,
     "start_time": "2021-12-21T12:12:09.425182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 15594/15594 [00:00<00:00, 41628.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000D23A521A</td>\n",
       "      <td>Some people belive that the so called \"face\" o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00066EA9880D</td>\n",
       "      <td>Driverless cars are exaclty what you would exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000E6DE9E817</td>\n",
       "      <td>Dear: Principal\\n\\nI am arguing against the po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001552828BD0</td>\n",
       "      <td>Would you be able to give your car up? Having ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text\n",
       "0  0000D23A521A  Some people belive that the so called \"face\" o...\n",
       "1  00066EA9880D  Driverless cars are exaclty what you would exp...\n",
       "2  000E6DE9E817  Dear: Principal\\n\\nI am arguing against the po...\n",
       "3  001552828BD0  Would you be able to give your car up? Having ...\n",
       "4  0016926B079C  I think that students would benefit from learn..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\n",
    "test_names, train_texts = [], []\n",
    "for f in tqdm(list(os.listdir('../input/feedback-prize-2021/train'))):\n",
    "    test_names.append(f.replace('.txt', ''))\n",
    "    train_texts.append(open('../input/feedback-prize-2021/train/' + f, 'r').read())\n",
    "train_text_df = pd.DataFrame({'id': test_names, 'text': train_texts})\n",
    "train_text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.123678,
     "end_time": "2021-12-21T12:12:48.368476",
     "exception": false,
     "start_time": "2021-12-21T12:12:48.244798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Convert Train Text to NER Labels\n",
    "We will now convert all text words into NER labels and save in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gradient": {
     "execution_count": 11,
     "id": "a6d20e74-8e25-4973-8953-f3b9acd81689",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15594, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E1FA876D6E6C</td>\n",
       "      <td>Dear Senator,\\n\\nI am writting this letter to ...</td>\n",
       "      <td>[O, O, B-Lead, I-Lead, I-Lead, I-Lead, I-Lead,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8AC1D6E165CD</td>\n",
       "      <td>Dear Principal, I believe in policy 2. Kids ar...</td>\n",
       "      <td>[O, O, B-Position, I-Position, I-Position, I-P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45EF6A4EDB1A</td>\n",
       "      <td>Summer projects are no fun, but they are a gre...</td>\n",
       "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B0070361406D</td>\n",
       "      <td>The author who wrote \"The challenge of Explori...</td>\n",
       "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>839F4F7F7DD7</td>\n",
       "      <td>Our school systems have seen many changes as t...</td>\n",
       "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  \\\n",
       "0  E1FA876D6E6C  Dear Senator,\\n\\nI am writting this letter to ...   \n",
       "1  8AC1D6E165CD  Dear Principal, I believe in policy 2. Kids ar...   \n",
       "2  45EF6A4EDB1A  Summer projects are no fun, but they are a gre...   \n",
       "3  B0070361406D  The author who wrote \"The challenge of Explori...   \n",
       "4  839F4F7F7DD7  Our school systems have seen many changes as t...   \n",
       "\n",
       "                                            entities  \n",
       "0  [O, O, B-Lead, I-Lead, I-Lead, I-Lead, I-Lead,...  \n",
       "1  [O, O, B-Position, I-Position, I-Position, I-P...  \n",
       "2  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...  \n",
       "3  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...  \n",
       "4  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not LOAD_TOKENS_FROM:\n",
    "    all_entities = []\n",
    "    for ii,i in enumerate(train_text_df.iterrows()):\n",
    "        if ii%100==0: print(ii,', ',end='')\n",
    "        total = i[1]['text'].split().__len__()\n",
    "        entities = [\"O\"]*total\n",
    "        for j in train_df[train_df['id'] == i[1]['id']].iterrows():\n",
    "            discourse = j[1]['discourse_type']\n",
    "            list_ix = [int(x) for x in j[1]['predictionstring'].split(' ')]\n",
    "            entities[list_ix[0]] = f\"B-{discourse}\"\n",
    "            for k in list_ix[1:]: entities[k] = f\"I-{discourse}\"\n",
    "        all_entities.append(entities)\n",
    "    train_text_df['entities'] = all_entities\n",
    "    train_text_df.to_csv('train_NER.csv',index=False)\n",
    "    \n",
    "else:\n",
    "    from ast import literal_eval\n",
    "    train_text_df = pd.read_csv(f'{LOAD_TOKENS_FROM}/train_NER.csv')\n",
    "    # pandas saves lists as string, we must convert back\n",
    "    train_text_df.entities = train_text_df.entities.apply(lambda x: literal_eval(x) )\n",
    "    \n",
    "print( train_text_df.shape )\n",
    "train_text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gradient": {
     "execution_count": 12,
     "id": "ee648d66-852f-47a5-a404-5890cde5c054",
     "kernelId": ""
    },
    "papermill": {
     "duration": 0.940609,
     "end_time": "2021-12-21T12:18:50.456125",
     "exception": false,
     "start_time": "2021-12-21T12:18:49.515516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CREATE DICTIONARIES THAT WE CAN USE DURING TRAIN AND INFER\n",
    "output_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n",
    "          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n",
    "\n",
    "labels_to_ids = {v:k for k,v in enumerate(output_labels)}\n",
    "ids_to_labels = {k:v for k,v in enumerate(output_labels)}\n",
    "disc_type_to_ids = {'Evidence':(11,12),'Claim':(5,6),'Lead':(1,2),'Position':(3,4),'Counterclaim':(7,8),'Rebuttal':(9,10),'Concluding Statement':(13,14)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gradient": {
     "execution_count": 13,
     "id": "181002ad-1436-4890-8230-149cd4e7fcc1",
     "kernelId": ""
    },
    "papermill": {
     "duration": 0.994404,
     "end_time": "2021-12-21T12:18:52.798977",
     "exception": false,
     "start_time": "2021-12-21T12:18:51.804573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-Lead': 1,\n",
       " 'I-Lead': 2,\n",
       " 'B-Position': 3,\n",
       " 'I-Position': 4,\n",
       " 'B-Claim': 5,\n",
       " 'I-Claim': 6,\n",
       " 'B-Counterclaim': 7,\n",
       " 'I-Counterclaim': 8,\n",
       " 'B-Rebuttal': 9,\n",
       " 'I-Rebuttal': 10,\n",
       " 'B-Evidence': 11,\n",
       " 'I-Evidence': 12,\n",
       " 'B-Concluding Statement': 13,\n",
       " 'I-Concluding Statement': 14}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.001889,
     "end_time": "2021-12-21T12:18:54.981896",
     "exception": false,
     "start_time": "2021-12-21T12:18:53.980007",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define the dataset function\n",
    "Below is our PyTorch dataset function. It always outputs tokens and attention. During training it also provides labels. And during inference it also provides word ids to help convert token predictions into word predictions.\n",
    "\n",
    "Note that we use `text.split()` and `is_split_into_words=True` when we convert train text to labeled train tokens. This is how the HugglingFace tutorial does it. However, this removes characters like `\\n` new paragraph. If you want your model to see new paragraphs, then we need to map words to tokens ourselves using `return_offsets_mapping=True`. See my TensorFlow notebook [here][1] for an example.\n",
    "\n",
    "Some of the following code comes from the example at HuggingFace [here][2]. However I think the code at that link is wrong. The HuggingFace original code is [here][3]. With the flag `LABEL_ALL` we can either label just the first subword token (when one word has more than one subword token). Or we can label all the subword tokens (with the word's label). In this notebook version, we label all the tokens. There is a Kaggle discussion [here][4]\n",
    "\n",
    "[1]: https://www.kaggle.com/cdeotte/tensorflow-longformer-ner-cv-0-617\n",
    "[2]: https://huggingface.co/docs/transformers/custom_datasets#tok_ner\n",
    "[3]: https://github.com/huggingface/transformers/blob/86b40073e9aee6959c8c85fcba89e47b432c4f4d/examples/pytorch/token-classification/run_ner.py#L371\n",
    "[4]: https://www.kaggle.com/c/feedback-prize-2021/discussion/296713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gradient": {
     "execution_count": 14,
     "id": "24ebb3cc-c13f-4356-b3b4-6e9dba4727ff",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "# Return an array that maps character index to index of word in list of split() words\n",
    "def split_mapping(unsplit):\n",
    "    splt = unsplit.split()\n",
    "    offset_to_wordidx = np.full(len(unsplit),-1)\n",
    "    txt_ptr = 0\n",
    "    for split_index, full_word in enumerate(splt):\n",
    "        while unsplit[txt_ptr:txt_ptr + len(full_word)] != full_word:\n",
    "            txt_ptr += 1\n",
    "        offset_to_wordidx[txt_ptr:txt_ptr + len(full_word)] = split_index\n",
    "        txt_ptr += len(full_word)\n",
    "    return offset_to_wordidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "gradient": {
     "execution_count": 15,
     "id": "7c2ec5a9-c120-4883-934d-41a83e3da1cc",
     "kernelId": ""
    },
    "papermill": {
     "duration": 0.934726,
     "end_time": "2021-12-21T12:18:56.852259",
     "exception": false,
     "start_time": "2021-12-21T12:18:55.917533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "  def __init__(self, dataframe, tokenizer, max_len, get_wids):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.get_wids = get_wids # for validation\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        # GET TEXT AND WORD LABELS \n",
    "        text = self.data.text[index]        \n",
    "        word_labels = self.data.entities[index] if not self.get_wids else None\n",
    "\n",
    "        # TOKENIZE TEXT\n",
    "        encoding = self.tokenizer(text,\n",
    "                             return_offsets_mapping=True, \n",
    "                             padding='max_length', \n",
    "                             truncation=True, \n",
    "                             max_length=self.max_len)\n",
    "        \n",
    "        word_ids = encoding.word_ids()  \n",
    "        split_word_ids = np.full(len(word_ids),-1)\n",
    "        offset_to_wordidx = split_mapping(text)\n",
    "        offsets = encoding['offset_mapping']\n",
    "        \n",
    "        # CREATE TARGETS AND MAPPING OF TOKENS TO SPLIT() WORDS\n",
    "        label_ids = []\n",
    "        # Iterate in reverse to label whitespace tokens until a Begin token is encountered\n",
    "        for token_idx, word_idx in reversed(list(enumerate(word_ids))):\n",
    "            \n",
    "            if word_idx is None:\n",
    "                if not self.get_wids: label_ids.append(-100)\n",
    "            else:\n",
    "                if offsets[token_idx] != (0,0):\n",
    "                    #Choose the split word that shares the most characters with the token if any\n",
    "                    split_idxs = offset_to_wordidx[offsets[token_idx][0]:offsets[token_idx][1]]\n",
    "                    split_index = stats.mode(split_idxs[split_idxs != -1]).mode[0] if len(np.unique(split_idxs)) > 1 else split_idxs[0]\n",
    "                    \n",
    "                    if split_index != -1: \n",
    "                        if not self.get_wids: label_ids.append( labels_to_ids[word_labels[split_index]] )\n",
    "                        split_word_ids[token_idx] = split_index\n",
    "                    else:\n",
    "                        # Even if we don't find a word, continue labeling 'I' tokens until a 'B' token is found\n",
    "                        if label_ids and label_ids[-1] != -100 and ids_to_labels[label_ids[-1]][0] == 'I':\n",
    "                            split_word_ids[token_idx] = split_word_ids[token_idx + 1]\n",
    "                            if not self.get_wids: label_ids.append(label_ids[-1])\n",
    "                        else:\n",
    "                            if not self.get_wids: label_ids.append(-100)\n",
    "                else:\n",
    "                    if not self.get_wids: label_ids.append(-100)\n",
    "        \n",
    "        encoding['labels'] = list(reversed(label_ids))\n",
    "\n",
    "        # CONVERT TO TORCH TENSORS\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        if self.get_wids: \n",
    "            item['wids'] = torch.as_tensor(split_word_ids)\n",
    "        \n",
    "        return item\n",
    "\n",
    "  def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.936225,
     "end_time": "2021-12-21T12:19:08.206923",
     "exception": false,
     "start_time": "2021-12-21T12:19:07.270698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create Train and Validation Dataloaders\n",
    "We will use the same train and validation subsets as my TensorFlow notebook [here][1]. Then we can compare results. And/or experiment with ensembling the validation fold predictions.\n",
    "\n",
    "[1]: https://www.kaggle.com/cdeotte/tensorflow-longformer-ner-cv-0-617"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "gradient": {
     "execution_count": 16,
     "id": "92015ad6-554b-4af0-b958-76b4def62ece",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15594 train texts. We will split 90% 10% for ensemble training.\n"
     ]
    }
   ],
   "source": [
    "# CHOOSE VALIDATION INDEXES (that match my TF notebook)\n",
    "IDS = train_df.id.unique()\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "if FOLD == 'half':\n",
    "    train_idx = np.random.choice(np.arange(len(IDS)),int(0.5*len(IDS)),replace=False)\n",
    "    valid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\n",
    "    \n",
    "elif FOLD is not None:\n",
    "    print('There are',len(IDS),'train texts. We will split 93% 7% for ensemble training.')\n",
    "    shuffled_ids = np.arange(len(IDS))\n",
    "    np.random.shuffle(shuffled_ids)\n",
    "\n",
    "    valid_len = int(.07 * len(IDS))\n",
    "    valid_idx = shuffled_ids[FOLD*valid_len:(FOLD+1)*valid_len]\n",
    "    train_idx = np.setdiff1d(np.arange(len(IDS)),valid_idx)\n",
    "    \n",
    "else:\n",
    "    print('There are',len(IDS),'train texts. We will split 90% 10% for ensemble training.')\n",
    "    train_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\n",
    "    valid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\n",
    "\n",
    "np.random.seed(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "gradient": {
     "execution_count": 17,
     "id": "ce4573a1-475a-45be-9bd8-bccef45cf5a6",
     "kernelId": ""
    },
    "papermill": {
     "duration": 0.953973,
     "end_time": "2021-12-21T12:19:10.088215",
     "exception": false,
     "start_time": "2021-12-21T12:19:09.134242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (15594, 3)\n",
      "TRAIN Dataset: (14034, 2)\n",
      "TEST Dataset: (1560, 3)\n"
     ]
    }
   ],
   "source": [
    "# CREATE TRAIN SUBSET AND VALID SUBSET\n",
    "data = train_text_df[['id','text', 'entities']]\n",
    "train_dataset = data.loc[data['id'].isin(IDS[train_idx]),['text', 'entities']].reset_index(drop=True)\n",
    "test_dataset = data.loc[data['id'].isin(IDS[valid_idx])].reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(data.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DOWNLOADED_MODEL_PATH) \n",
    "training_set = dataset(train_dataset, tokenizer, config['max_length'], False)\n",
    "testing_set = dataset(test_dataset, tokenizer, config['max_length'], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "gradient": {
     "execution_count": 18,
     "id": "3b670879-0d5d-44c6-950d-780518c2b3bf",
     "kernelId": ""
    },
    "papermill": {
     "duration": 0.955464,
     "end_time": "2021-12-21T12:19:12.022567",
     "exception": false,
     "start_time": "2021-12-21T12:19:11.067103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TRAIN DATASET AND VALID DATASET\n",
    "train_params = {'batch_size': config['train_batch_size'],\n",
    "                'shuffle': True,\n",
    "                'num_workers': 2,\n",
    "                'pin_memory':True\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': config['valid_batch_size'],\n",
    "                'shuffle': False,\n",
    "                'num_workers': 2,\n",
    "                'pin_memory':True\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "# TEST DATASET\n",
    "test_texts_set = dataset(test_texts, tokenizer, config['max_length'], True)\n",
    "test_texts_loader = DataLoader(test_texts_set, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "The PyTorch train function is taken from Raghavendrakotala's great notebook [here][1]. I assume it uses a masked loss which avoids computing loss when target is `-100`. If not, we need to update this.\n",
    "\n",
    "[1]: https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "gradient": {
     "execution_count": 19,
     "id": "6527e498-bd3e-4e2b-90e9-ca99d795dd3a",
     "kernelId": ""
    },
    "papermill": {
     "duration": 1.00345,
     "end_time": "2021-12-21T12:19:31.294225",
     "exception": false,
     "start_time": "2021-12-21T12:19:30.290775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\n",
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    #tr_preds, tr_labels = [], []\n",
    "    \n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        \n",
    "        ids = batch['input_ids'].to(config['device'], dtype = torch.long)\n",
    "        mask = batch['attention_mask'].to(config['device'], dtype = torch.long)\n",
    "        labels = batch['labels'].to(config['device'], dtype = torch.long)\n",
    "\n",
    "        with amp.autocast():\n",
    "            loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels,\n",
    "                                   return_dict=False)\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += labels.size(0)\n",
    "        \n",
    "        if idx % 200==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training loss after {idx:04d} training steps: {loss_step}\")\n",
    "           \n",
    "        # compute training accuracy\n",
    "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        \n",
    "        # only compute accuracy at active labels\n",
    "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
    "        \n",
    "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        \n",
    "        #tr_labels.extend(labels)\n",
    "        #tr_preds.extend(predictions)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "    \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=config['max_grad_norm']\n",
    "        )\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "gradient": {
     "execution_count": 20,
     "id": "06cd88d7-637e-426c-94cc-78d0b8702cf3",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "# CREATE MODEL\n",
    "scaler = amp.GradScaler()\n",
    "config_model = AutoConfig.from_pretrained(DOWNLOADED_MODEL_PATH+'/config.json') \n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "                   DOWNLOADED_MODEL_PATH+'/pytorch_model.bin',config=config_model)\n",
    "model.to(config['device'])\n",
    "optimizer = AdamW(params=model.parameters(), lr=config['learning_rates'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "gradient": {
     "execution_count": 21,
     "id": "6c331f3a-7039-49ff-a97e-70ee5647c658",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', '.*__floordiv__ is deprecated.*',)\n",
    "\n",
    "# LOOP TO TRAIN MODEL (or load model)\n",
    "if not LOAD_MODEL_FROM:\n",
    "    for epoch in range(config['epochs']):\n",
    "        \n",
    "        print(f\"### Training epoch: {epoch + 1}\")\n",
    "        for g in optimizer.param_groups: \n",
    "            g['lr'] = config['learning_rates'][epoch]\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'### LR = {lr}\\n')\n",
    "        \n",
    "        train(epoch)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    torch.save(model.state_dict(), f'bigbird_v{VER}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Validation Code\n",
    "We will infer in batches using our data loader which is faster than inferring one text at a time with a for-loop. The metric code is taken from Rob Mulla's great notebook [here][2]. Our model achieves validation F1 score 0.615! \n",
    "\n",
    "During inference our model will make predictions for each subword token. Some single words consist of multiple subword tokens. In the code below, we use a word's first subword token prediction as the label for the entire word. We can try other approaches, like averaging all subword predictions or taking `B` labels before `I` labels etc.\n",
    "\n",
    "[1]: https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\n",
    "[2]: https://www.kaggle.com/robikscube/student-writing-competition-twitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "gradient": {
     "execution_count": 22,
     "id": "93df9ccc-f7ff-4cb2-8e20-8030026fff42",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Returns per-word, mean class prediction probability over all tokens corresponding to each word\n",
    "def inference(data_loader, model_ids):\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    ensemble_preds = np.zeros((len(data_loader.dataset), config['max_length'], len(labels_to_ids)), dtype=np.float32)\n",
    "    wids = np.full((len(data_loader.dataset), config['max_length']), -100)\n",
    "    for model_i, model_id in enumerate(model_ids):\n",
    "        \n",
    "        model.load_state_dict(torch.load(f'{LOAD_MODEL_FROM}/ensemble-{model_id}.pt', map_location=config['device']))\n",
    "        \n",
    "        # put model in training mode\n",
    "        model.eval()\n",
    "        for batch_i, batch in enumerate(data_loader):\n",
    "            \n",
    "            if model_i == 0: wids[batch_i*config['valid_batch_size']:(batch_i+1)*config['valid_batch_size']] = batch['wids'].numpy()\n",
    "\n",
    "            # MOVE BATCH TO GPU AND INFER\n",
    "            ids = batch[\"input_ids\"].to(config['device'])\n",
    "            mask = batch[\"attention_mask\"].to(config['device'])\n",
    "            with amp.autocast():\n",
    "                outputs = model(ids, attention_mask=mask, return_dict=False)\n",
    "            all_preds = torch.nn.functional.softmax(outputs[0], dim=2).cpu().detach().numpy() \n",
    "            ensemble_preds[batch_i*config['valid_batch_size']:(batch_i+1)*config['valid_batch_size']] += all_preds\n",
    "            \n",
    "            del ids\n",
    "            del mask\n",
    "            del outputs\n",
    "            del all_preds\n",
    "            \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "            \n",
    "    ensemble_preds /= len(model_ids)\n",
    "    predictions = []\n",
    "    # INTERATE THROUGH EACH TEXT AND GET PRED\n",
    "    for text_i in range(ensemble_preds.shape[0]):\n",
    "        token_preds = ensemble_preds[text_i]\n",
    "        \n",
    "        prediction = []\n",
    "        previous_word_idx = -1\n",
    "        prob_buffer = []\n",
    "        word_ids = wids[text_i][wids[text_i] != -100]\n",
    "        for idx,word_idx in enumerate(word_ids):                            \n",
    "            if word_idx == -1:\n",
    "                pass\n",
    "            elif word_idx != previous_word_idx:              \n",
    "                if prob_buffer:\n",
    "                    prediction.append(np.mean(prob_buffer, dtype=np.float32, axis=0))\n",
    "                    prob_buffer = []\n",
    "                prob_buffer.append(token_preds[idx])\n",
    "                previous_word_idx = word_idx\n",
    "            else: \n",
    "                prob_buffer.append(token_preds[idx])\n",
    "        prediction.append(np.mean(prob_buffer, dtype=np.float32, axis=0))\n",
    "        predictions.append(prediction)\n",
    "            \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "gradient": {
     "execution_count": 23,
     "id": "cadc897b-9143-4b68-960a-0b2d761b63cb",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "# from Rob Mulla @robikscube\n",
    "# https://www.kaggle.com/robikscube/student-writing-competition-twitch\n",
    "def calc_overlap(row):\n",
    "    \"\"\"\n",
    "    Calculates the overlap between prediction and\n",
    "    ground truth and overlap percentages used for determining\n",
    "    true positives.\n",
    "    \"\"\"\n",
    "    set_pred = set(row.predictionstring_pred.split(' '))\n",
    "    set_gt = set(row.predictionstring_gt.split(' '))\n",
    "    # Length of each and intersection\n",
    "    len_gt = len(set_gt)\n",
    "    len_pred = len(set_pred)\n",
    "    inter = len(set_gt.intersection(set_pred))\n",
    "    overlap_1 = inter / len_gt\n",
    "    overlap_2 = inter/ len_pred\n",
    "    return [overlap_1, overlap_2]\n",
    "\n",
    "\n",
    "def score_feedback_comp(pred_df, gt_df):\n",
    "    \"\"\"\n",
    "    A function that scores for the kaggle\n",
    "        Student Writing Competition\n",
    "        \n",
    "    Uses the steps in the evaluation page here:\n",
    "        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n",
    "    \"\"\"\n",
    "    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n",
    "        .reset_index(drop=True).copy()\n",
    "    pred_df = pred_df[['id','class','predictionstring']] \\\n",
    "        .reset_index(drop=True).copy()\n",
    "    pred_df['pred_id'] = pred_df.index\n",
    "    gt_df['gt_id'] = gt_df.index\n",
    "    # Step 1. all ground truths and predictions for a given class are compared.\n",
    "    joined = pred_df.merge(gt_df,\n",
    "                           left_on=['id','class'],\n",
    "                           right_on=['id','discourse_type'],\n",
    "                           how='outer',\n",
    "                           suffixes=('_pred','_gt')\n",
    "                          )\n",
    "    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n",
    "    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n",
    "\n",
    "    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n",
    "\n",
    "    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n",
    "    # and the overlap between the prediction and the ground truth >= 0.5,\n",
    "    # the prediction is a match and considered a true positive.\n",
    "    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
    "    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n",
    "    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n",
    "\n",
    "\n",
    "    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n",
    "    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n",
    "    tp_pred_ids = joined.query('potential_TP') \\\n",
    "        .sort_values('max_overlap', ascending=False) \\\n",
    "        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n",
    "\n",
    "    # 3. Any unmatched ground truths are false negatives\n",
    "    # and any unmatched predictions are false positives.\n",
    "    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n",
    "\n",
    "    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n",
    "    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n",
    "\n",
    "    # Get numbers of each type\n",
    "    TP = len(tp_pred_ids)\n",
    "    FP = len(fp_pred_ids)\n",
    "    FN = len(unmatched_gt_ids)\n",
    "    #calc microf1\n",
    "    my_f1_score = TP / (TP + 0.5*(FP+FN))\n",
    "    return my_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the per-word, mean class probability predictions from BigBird for validation and submit sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "gradient": {
     "execution_count": 24,
     "id": "7597203a-32ac-477b-951e-befcb89c709a",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with BigBird...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "valid = train_df.loc[train_df['id'].isin(IDS[valid_idx])]\n",
    "\n",
    "print('Predicting with BigBird...')\n",
    "if not SUBMISSION:\n",
    "    try:\n",
    "        with open( KAGGLE_CACHE + \"/valid_preds.p\", \"rb\" ) as validFile:\n",
    "            valid_word_preds = pickle.load( validFile )    \n",
    "    except:\n",
    "        valid_word_preds = inference(testing_loader, ENSEMBLE_IDS)\n",
    "else: valid_word_preds = []\n",
    "        \n",
    "test_word_preds = inference(test_texts_loader, ENSEMBLE_IDS)\n",
    "    \n",
    "with open( cache + \"/valid_preds.p\", \"wb\" ) as validFile:\n",
    "    pickle.dump( valid_word_preds, validFile )\n",
    "print('Done.')\n",
    "\n",
    "uniqueValidGroups = range(len(valid_word_preds))\n",
    "uniqueSubmitGroups = range(len(test_word_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Datasets\n",
    "We will create datasets that, instead of describing individual words or tokens, describes sequences of words. Within some heuristic constraints, every possible sub-sequence of words in a text will converted to a dataset sample with the following attributes:\n",
    "\n",
    "* features- sequence length, position, and various kinds of class probability predictions/statistics\n",
    "* labels- whether the sequence matches exactly a discourse instance\n",
    "* truePos- whether the sequence matches a discourse instance by competition criteria for true positive\n",
    "* groups- the integer index of the text where the sequence is found\n",
    "* wordRanges- the start and end word index of the sequence in the text\n",
    "\n",
    "Sequence datasets are generated for each discourse type and for validation and submission datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "gradient": {
     "execution_count": 25,
     "id": "268caa30-cdb4-4dbe-873e-60d40637e003",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from bisect import bisect_left\n",
    "\n",
    "# Percentile code taken from https://www.kaggle.com/vuxxxx/tensorflow-longformer-ner-postprocessing\n",
    "# Thank Vu!\n",
    "#\n",
    "# Use 99.5% of the distribution of lengths for a disourse type as maximum. \n",
    "# Increasing this constraint makes this step slower but generally increases performance.\n",
    "MAX_SEQ_LEN = {}\n",
    "train_df['len'] = train_df['predictionstring'].apply(lambda x:len(x.split()))\n",
    "max_lens = train_df.groupby('discourse_type')['len'].quantile(.995)\n",
    "for disc_type in disc_type_to_ids:\n",
    "    MAX_SEQ_LEN[disc_type] = int(max_lens[disc_type])\n",
    "\n",
    "#The minimum probability prediction for a 'B'egin class for which we will evaluate a word sequence\n",
    "MIN_BEGIN_PROB = {\n",
    "    'Claim': .35,\n",
    "    'Concluding Statement': .15,\n",
    "    'Counterclaim': .04,\n",
    "    'Evidence': .1,\n",
    "    'Lead': .32,\n",
    "    'Position': .25,\n",
    "    'Rebuttal': .01,\n",
    "}\n",
    "        \n",
    "class SeqDataset(object):\n",
    "    \n",
    "    def __init__(self, features, labels, groups, wordRanges, truePos):\n",
    "        \n",
    "        self.features = np.array(features, dtype=np.float32)\n",
    "        self.labels = np.array(labels)\n",
    "        self.groups = np.array(groups, dtype=np.int16)\n",
    "        self.wordRanges = np.array(wordRanges, dtype=np.int16)\n",
    "        self.truePos = np.array(truePos)\n",
    "\n",
    "# Adapted from https://stackoverflow.com/questions/60467081/linear-interpolation-in-numpy-quantile\n",
    "# This is used to prevent re-sorting to compute quantile for every sequence.\n",
    "def sorted_quantile(array, q):\n",
    "    array = np.array(array)\n",
    "    n = len(array)\n",
    "    index = (n - 1) * q\n",
    "    left = np.floor(index).astype(int)\n",
    "    fraction = index - left\n",
    "    right = left\n",
    "    right = right + (fraction > 0).astype(int)\n",
    "    i, j = array[left], array[right]\n",
    "    return i + (j - i) * fraction\n",
    "        \n",
    "def seq_dataset(disc_type, pred_indices=None, submit=False):\n",
    "    word_preds = valid_word_preds if not submit else test_word_preds\n",
    "    window = pred_indices if pred_indices else range(len(word_preds))\n",
    "    X = np.empty((int(1e6),13), dtype=np.float32)\n",
    "    X_ind = 0\n",
    "    y = []\n",
    "    truePos = []\n",
    "    wordRanges = []\n",
    "    groups = []\n",
    "    for text_i in tqdm(window):\n",
    "        text_preds = np.array(word_preds[text_i])\n",
    "        num_words = len(text_preds)\n",
    "        disc_begin, disc_inside = disc_type_to_ids[disc_type]\n",
    "        \n",
    "        # The probability that a word corresponds to either a 'B'-egin or 'I'-nside token for a class\n",
    "        prob_or = lambda word_preds: (1-(1-word_preds[:,disc_begin]) * (1-word_preds[:,disc_inside]))\n",
    "        \n",
    "        if not submit:\n",
    "            gt_idx = set()\n",
    "            gt_arr = np.zeros(num_words, dtype=int)\n",
    "            text_gt = valid.loc[valid.id == test_dataset.id.values[text_i]]\n",
    "            disc_gt = text_gt.loc[text_gt.discourse_type == disc_type]\n",
    "            \n",
    "            # Represent the discourse instance locations in a hash set and an integer array for speed\n",
    "            for row_i, row in enumerate(disc_gt.iterrows()):\n",
    "                splt = row[1]['predictionstring'].split()\n",
    "                start, end = int(splt[0]), int(splt[-1]) + 1\n",
    "                gt_idx.add((start, end))\n",
    "                gt_arr[start:end] = row_i + 1\n",
    "            gt_lens = np.bincount(gt_arr)\n",
    "        \n",
    "        # Iterate over every sub-sequence in the text\n",
    "        quants = np.linspace(0,1,7)\n",
    "        prob_begins = np.copy(text_preds[:,disc_begin])\n",
    "        min_begin = MIN_BEGIN_PROB[disc_type]\n",
    "        for pred_start in range(num_words):\n",
    "            prob_begin = prob_begins[pred_start]\n",
    "            if prob_begin > min_begin:\n",
    "                begin_or_inside = []\n",
    "                for pred_end in range(pred_start+1,min(num_words+1, pred_start+MAX_SEQ_LEN[disc_type]+1)):\n",
    "                    \n",
    "                    new_prob = prob_or(text_preds[pred_end-1:pred_end])\n",
    "                    insert_i = bisect_left(begin_or_inside, new_prob)\n",
    "                    begin_or_inside.insert(insert_i, new_prob[0])\n",
    "\n",
    "                    # Generate features for a word sub-sequence\n",
    "\n",
    "                    # The length and position of start/end of the sequence\n",
    "                    features = [pred_end - pred_start, pred_start / float(num_words), pred_end / float(num_words)]\n",
    "                    \n",
    "                    # 7 evenly spaced quantiles of the distribution of relevant class probabilities for this sequence\n",
    "                    features.extend(list(sorted_quantile(begin_or_inside, quants)))\n",
    "\n",
    "                    # The probability that words on either edge of the current sub-sequence belong to the class of interest\n",
    "                    features.append(prob_or(text_preds[pred_start-1:pred_start])[0] if pred_start > 0 else 0)\n",
    "                    features.append(prob_or(text_preds[pred_end:pred_end+1])[0] if pred_end < num_words else 0)\n",
    "\n",
    "                    # The probability that the first word corresponds to a 'B'-egin token\n",
    "                    features.append(text_preds[pred_start,disc_begin])\n",
    "\n",
    "                    exact_match = (pred_start, pred_end) in gt_idx if not submit else None\n",
    "\n",
    "                    if not submit:\n",
    "                        true_pos = False\n",
    "                        for match_cand, count in Counter(gt_arr[pred_start:pred_end]).most_common(2):\n",
    "                            if match_cand != 0 and count / float(pred_end - pred_start) >= .5 and float(count) / gt_lens[match_cand] >= .5: true_pos = True\n",
    "                    else: true_pos = None\n",
    "\n",
    "                    # For efficiency, use a numpy array instead of a list that doubles in size when full to conserve constant \"append\" time complexity\n",
    "                    if X_ind >= X.shape[0]:\n",
    "                        new_X = np.empty((X.shape[0]*2,13), dtype=np.float32)\n",
    "                        new_X[:X.shape[0]] = X\n",
    "                        X = new_X\n",
    "                    X[X_ind] = features\n",
    "                    X_ind += 1\n",
    "                    \n",
    "                    y.append(exact_match)\n",
    "                    truePos.append(true_pos)\n",
    "                    wordRanges.append((np.int16(pred_start), np.int16(pred_end)))\n",
    "                    groups.append(np.int16(text_i))\n",
    "\n",
    "    return SeqDataset(X[:X_ind], y, groups, wordRanges, truePos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "gradient": {
     "execution_count": 26,
     "id": "a03de634-8afe-4e9f-8634-2c1e932aa64c",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making validation sequence datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 1560/1560 [00:10<00:00, 155.14it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1560/1560 [00:12<00:00, 129.46it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 1560/1560 [00:13<00:00, 114.17it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 1560/1560 [00:18<00:00, 82.71it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 1560/1560 [00:21<00:00, 71.35it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 1560/1560 [00:28<00:00, 54.83it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 1560/1560 [03:00<00:00,  8.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Making submit sequence datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 688.29it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 378.88it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 191.72it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 176.83it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 87.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 76.24it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 17.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Manager\n",
    "\n",
    "manager = Manager()\n",
    "\n",
    "\n",
    "def sequenceDataset(disc_type, submit=False):\n",
    "    if not submit: validSeqSets[disc_type] = seq_dataset(disc_type) if not SUBMISSION else None\n",
    "    else: submitSeqSets[disc_type] = seq_dataset(disc_type, submit=True)\n",
    "\n",
    "try:\n",
    "    with open( KAGGLE_CACHE + \"/valid_seqds.p\", \"rb\" ) as validFile:\n",
    "        validSeqSets = pickle.load( validFile )  \n",
    "except:\n",
    "    print('Making validation sequence datasets...')\n",
    "    validSeqSets = manager.dict()\n",
    "    Parallel(n_jobs=-1, backend='multiprocessing')(\n",
    "            delayed(sequenceDataset)(disc_type, False) \n",
    "           for disc_type in disc_type_to_ids\n",
    "        )\n",
    "    print('Done.')\n",
    "    \n",
    "    \n",
    "print('Making submit sequence datasets...')\n",
    "submitSeqSets = manager.dict()\n",
    "Parallel(n_jobs=-1, backend='multiprocessing')(\n",
    "        delayed(sequenceDataset)(disc_type, True) \n",
    "       for disc_type in disc_type_to_ids\n",
    "    )\n",
    "print('Done.')\n",
    "    \n",
    "with open( cache + \"/valid_seqds.p\", \"wb\" ) as validFile:\n",
    "    pickle.dump( dict(validSeqSets), validFile )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEGATIVE_SAMPLE_RATIO = 1\n",
    "\n",
    "# Downsample negative samples to 1:1 for efficiency/ease. There are many samples, and performance increase was observed.\n",
    "def resample(y):\n",
    "    global resample_call\n",
    "    counts = np.bincount(y)\n",
    "    np.random.seed((resample_call+counts[0]) % 2**32)\n",
    "    \n",
    "    neg_sample_count = NEGATIVE_SAMPLE_RATIO*counts[1]\n",
    "    indices = np.concatenate((\n",
    "        np.random.choice(np.arange(len(y))[y==0], neg_sample_count, replace=False),\n",
    "        np.arange(len(y))[y==1]\n",
    "    ))\n",
    "    indices.sort()\n",
    "    resample_call += 1\n",
    "    return indices\n",
    "\n",
    "resample_call = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Classifier Tuning\n",
    "\n",
    "Every discorse type/class is trained in separate, parallel optimization loops (if you have enough CPUs).\n",
    "\n",
    "During a training iteration, the validation set is split into 8 folds. For every fold, a classifier is trained on 7 folds to predict the probability that a word sub-sequence is a true positive in the remaining fold. To compose a set of predictions for a text, those sub-sequences with the highest predicted probability of being a true positive are included iteratively, so long as they do not intersect with previously included sub-sequences. Sub-sequences are no longer included when their predicted probability is beneath a threshold. Tuing this per-class theshold is the objective of this tuning stage.\n",
    "\n",
    "Once a set of predicted sub-sequences is composed for each text in each fold, the competition Macro F1 score is computed for the entire validation set. A noisy optimization algorithm, `skopt.gp_minimize`, is used to find the optimal probability threshold described above for each class with minimal iterations. After tuning is complete, the classifier in each fold is saved to disc. The mean probability predictions of this ensemble of classifiers is used during submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "gradient": {
     "execution_count": 27,
     "id": "cc2e7c81-2347-4b3d-90c6-2b52d2ffa036",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Manager\n",
    "from sklearn.model_selection import cross_val_score, GroupKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from skopt.space import Real\n",
    "from skopt import gp_minimize\n",
    "import sys\n",
    "import xgboost\n",
    "\n",
    "NUM_FOLDS = 8\n",
    "\n",
    "warnings.filterwarnings('ignore', '.*ragged nested sequences*',)\n",
    "\n",
    "seq_cache = {} # For each fold and each text. cache score predictions sorted by score\n",
    "clfs = []  # Each fold will add its classifier here\n",
    "# Predict sub-sequences for a discourse type and set of train/test texts\n",
    "def predict_strings(disc_type, probThresh, test_groups, train_ind=None, submit=False):\n",
    "    string_preds = []\n",
    "    validSeqDs = validSeqSets[disc_type]\n",
    "    submitSeqDs = submitSeqSets[disc_type]\n",
    "    \n",
    "    # Average the probability predictions of a set of classifiers\n",
    "    get_tp_prob = lambda testDs, classifiers: np.mean([clf.predict_proba(testDs.features)[:,1] for clf in classifiers], axis=0) if testDs.features.shape[0] > 0 else np.array([])\n",
    "    \n",
    "    if not submit:\n",
    "        # Point to validation set values\n",
    "        predict_df = test_dataset\n",
    "        text_df = train_text_df\n",
    "        groupIdx = np.isin(validSeqDs.groups, test_groups)\n",
    "        testDs = SeqDataset(validSeqDs.features[groupIdx], validSeqDs.labels[groupIdx], validSeqDs.groups[groupIdx], validSeqDs.wordRanges[groupIdx], validSeqDs.truePos[groupIdx])\n",
    "        \n",
    "        # Cache the classifier predictions to speed up tuning iterations\n",
    "        seq_key = (disc_type, tuple(test_groups), tuple(train_ind))\n",
    "        if seq_key in seq_cache:\n",
    "            text_to_seq = seq_cache[seq_key]\n",
    "        else:\n",
    "\n",
    "            clf = xgboost.XGBClassifier(\n",
    "                learning_rate = 0.05,\n",
    "                n_estimators=200,\n",
    "                max_depth=7,\n",
    "                min_child_weight=5,\n",
    "                gamma=0,\n",
    "                subsample=0.7,\n",
    "                reg_alpha=.0005,\n",
    "                colsample_bytree=0.6,\n",
    "                scale_pos_weight=1,\n",
    "                use_label_encoder=False,\n",
    "                eval_metric='logloss',\n",
    "                tree_method='hist'\n",
    "            )\n",
    "            \n",
    "            resampled = resample(validSeqDs.truePos[train_ind])\n",
    "            clf.fit(validSeqDs.features[train_ind][resampled],validSeqDs.truePos[train_ind][resampled])\n",
    "            clfs.append(clf)\n",
    "            prob_tp = get_tp_prob(testDs, [clf])\n",
    "        \n",
    "    else:\n",
    "        # Point to submission set values\n",
    "        predict_df = test_texts\n",
    "        text_df = test_texts\n",
    "        groupIdx = np.isin(submitSeqDs.groups, test_groups)\n",
    "        testDs = SeqDataset(submitSeqDs.features[groupIdx], submitSeqDs.labels[groupIdx], submitSeqDs.groups[groupIdx], submitSeqDs.wordRanges[groupIdx], submitSeqDs.truePos[groupIdx])\n",
    "        \n",
    "        # Classifiers are always loaded from disc during submission\n",
    "        with open( f\"../input/seqclassifiers6/{disc_type}_clf.p\", \"rb\" ) as clfFile:\n",
    "            classifiers = pickle.load( clfFile )  \n",
    "        prob_tp = get_tp_prob(testDs, classifiers)\n",
    "        \n",
    "    if submit or seq_key not in seq_cache:\n",
    "        text_to_seq = {}\n",
    "        for text_idx in test_groups:\n",
    "            # The probability of true positive and (start,end) of each sub-sequence in the curent text\n",
    "            prob_tp_curr = prob_tp[testDs.groups == text_idx]\n",
    "            word_ranges_curr = testDs.wordRanges[testDs.groups == text_idx]\n",
    "            sorted_seqs = list(reversed(sorted(zip(prob_tp_curr, [tuple(wr) for wr in word_ranges_curr]))))\n",
    "            text_to_seq[text_idx] = sorted_seqs\n",
    "        if not submit: seq_cache[seq_key] = text_to_seq\n",
    "    \n",
    "    for text_idx in test_groups:\n",
    "        \n",
    "        i = 1\n",
    "        split_text = text_df.loc[text_df.id == predict_df.id.values[text_idx]].iloc[0].text.split()\n",
    "        \n",
    "        # Start and end word indices of sequence candidates kept in sorted order for efficiency\n",
    "        starts = []\n",
    "        ends = []\n",
    "        \n",
    "        # Include the sub-sequence predictions in order of predicted probability\n",
    "        for prob, wordRange in text_to_seq[text_idx]:\n",
    "            \n",
    "            # Until the predicted probability is lower than the tuned threshold\n",
    "            if prob < probThresh: break\n",
    "                \n",
    "            # Binary search already-placed word sequence intervals, and insert the new word sequence interval if it does not intersect an existing interval.\n",
    "            insert = bisect_left(starts, wordRange[0])\n",
    "            if (insert == 0 or ends[insert-1] <= wordRange[0]) and (insert == len(starts) or starts[insert] >= wordRange[1]):\n",
    "                starts.insert(insert, wordRange[0])\n",
    "                ends.insert(insert, wordRange[1])\n",
    "                string_preds.append((predict_df.id.values[text_idx], disc_type, ' '.join(map(str, list(range(wordRange[0], wordRange[1]))))))\n",
    "                i += 1     \n",
    "    return string_preds\n",
    "\n",
    "def sub_df(string_preds):\n",
    "    return pd.DataFrame(string_preds, columns=['id','class','predictionstring'])\n",
    "    \n",
    "# Convert skopt's uniform distribution over the tuning threshold to a distribution that exponentially decays from 100% to 0%\n",
    "def prob_thresh(x): \n",
    "    return .01*(100-np.exp(100*x))\n",
    "\n",
    "# Convert back to the scalar supplied by skopt\n",
    "def skopt_thresh(x): \n",
    "    return np.log((x/.01-100.)/-1.)/100.\n",
    "    \n",
    "# This function is called every tuning iteration.\n",
    "# It takes the probability threshold as input and returns Macro F1\n",
    "def score_fmin(arr, disc_type):\n",
    "    validSeqDs = validSeqSets[disc_type]\n",
    "    string_preds = []\n",
    "    folds = np.array(list(GroupKFold(n_splits=NUM_FOLDS).split(validSeqDs.features, groups=validSeqDs.groups)))\n",
    "    gt_indices = []\n",
    "    for ind in folds[:,1]: gt_indices.extend(ind)\n",
    "        \n",
    "    # Texts that have no samples in our dataset for this class\n",
    "    unsampled_texts = np.array(np.array_split(list(set(uniqueValidGroups).difference(set(np.unique(validSeqDs.groups)))), NUM_FOLDS))\n",
    "    \n",
    "    gt_texts = test_dataset.id.values[np.unique(validSeqDs.groups[np.array(gt_indices, dtype=int)]).astype(int)]\n",
    "    \n",
    "    # Generate predictions from each fold of the validation set\n",
    "    for fold_i, (train_ind, test_ind) in enumerate(folds):\n",
    "        string_preds.extend(predict_strings(disc_type, prob_thresh(arr[0]), np.concatenate((np.unique(validSeqDs.groups[test_ind]), unsampled_texts[fold_i])).astype(int), train_ind))\n",
    "    boost_df = sub_df(list(string_preds))\n",
    "    gt_df = valid.loc[np.bitwise_and(valid['discourse_type']==disc_type, valid.id.isin(gt_texts))].copy()\n",
    "    f1 = score_feedback_comp(boost_df.copy(), gt_df)\n",
    "    return -f1\n",
    "\n",
    "\n",
    "def train_seq_clfs(disc_type):\n",
    "    # The optimization bounds on the tuned probability threshold \n",
    "    space_start = skopt_thresh(.999)\n",
    "    space_end = skopt_thresh(0)\n",
    "    space  = [Real(space_start,space_end)]\n",
    "    \n",
    "    # Minimize F1\n",
    "    score_fmin_disc = lambda arr: score_fmin(arr, disc_type)\n",
    "    res_gp = gp_minimize(score_fmin_disc, space, n_calls=100, x0=[skopt_thresh(.5)])\n",
    "    \n",
    "    # Use the gaussian approximation of f(threshold) -> F1 to select the minima\n",
    "    thresh_cand = np.rot90([np.linspace(0,1,1000)])\n",
    "    cand_scores = res_gp.models[-1].predict(thresh_cand)\n",
    "    best_thresh_raw = space_start + (space_end - space_start)*thresh_cand[np.argmin(cand_scores)][0]\n",
    "    best_thresh = prob_thresh(best_thresh_raw)\n",
    "    exp_score = -np.min(cand_scores)\n",
    "    \n",
    "    # Make predictions at the inferred function minimum\n",
    "    pred_thresh_score = -score_fmin_disc([best_thresh_raw])\n",
    "    \n",
    "    # And the best iteration in the optimization run\n",
    "    best_iter_score = -score_fmin_disc(res_gp.x)\n",
    "    \n",
    "    # Save the trained classifiers to disc\n",
    "    with open( f\"{disc_type}_clf.p\", \"wb\" ) as clfFile:\n",
    "        pickle.dump( clfs, clfFile )\n",
    "        \n",
    "    # Save the tuning run results to file\n",
    "    with open( f\"{disc_type}_res.p\", \"wb\" ) as resFile:\n",
    "        pickle.dump( \n",
    "            {\n",
    "                'pred_thresh': best_thresh,  # The location of the minimum of the gaussian function inferred by skopt\n",
    "                'min_thresh': prob_thresh(res_gp.x[0]),  # The threshold which produces the best score\n",
    "                'pred_score': exp_score,  # The minimum of the gaussian function inferred by skopt\n",
    "                'min_score': best_iter_score, # The best score in the tuning run\n",
    "                'pred_thresh_score': pred_thresh_score  # The score produced by 'pred_thresh'\n",
    "            }, \n",
    "            resFile \n",
    "        )\n",
    "    print('Done training', disc_type)\n",
    "    \n",
    "if TRAIN_SEQ_CLASSIFIERS and not SUBMISSION:\n",
    "    print('Training sequence classifiers... (This takes a long time.)')\n",
    "    Parallel(n_jobs=-1, backend='multiprocessing')(\n",
    "            delayed(train_seq_clfs)(disc_type) \n",
    "           for disc_type in disc_type_to_ids\n",
    "    )\n",
    "    print('Done training all sequence classifiers.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tuned probability thresholds from tuning result files, and make sub-sequence predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "gradient": {
     "execution_count": 28,
     "id": "087e600e-f15f-4154-8c26-6a8dabcfb73b",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence {'pred_thresh': 0.5728006033693174, 'min_thresh': 0.5739916597129654, 'pred_score': 0.8160563448783488, 'min_score': 0.8162833995873798, 'pred_thresh_score': 0.816208466433584}\n",
      "Claim {'pred_thresh': 0.4706733941639398, 'min_thresh': 0.4703150223755293, 'pred_score': 0.7160116735944848, 'min_score': 0.7162027123483227, 'pred_thresh_score': 0.7160805215818778}\n",
      "Lead {'pred_thresh': 0.5698364241893157, 'min_thresh': 0.5661079135342116, 'pred_score': 0.9408220046761397, 'min_score': 0.9412604042806183, 'pred_thresh_score': 0.9412464319695528}\n",
      "Position {'pred_thresh': 0.6095264763114393, 'min_thresh': 0.6116913376465217, 'pred_score': 0.8475453519278474, 'min_score': 0.8479389312977099, 'pred_thresh_score': 0.8476800976800977}\n",
      "Counterclaim {'pred_thresh': 0.7830616481614773, 'min_thresh': 0.7834795397682026, 'pred_score': 0.7386467434272197, 'min_score': 0.7391565060963422, 'pred_thresh_score': 0.7390087929656275}\n",
      "Rebuttal {'pred_thresh': 0.8485810674695611, 'min_thresh': 0.8483623507513574, 'pred_score': 0.6908241043240773, 'min_score': 0.6912043301759134, 'pred_thresh_score': 0.6908500270709258}\n",
      "Concluding Statement {'pred_thresh': 0.4706733941639398, 'min_thresh': 0.46741077293119887, 'pred_score': 0.9313246421532366, 'min_score': 0.9315045719035744, 'pred_thresh_score': 0.9314043402344724}\n"
     ]
    }
   ],
   "source": [
    "thresholds = {}\n",
    "for disc_type in disc_type_to_ids:\n",
    "    with open( f\"../input/seqclassifiers6/{disc_type}_res.p\", \"rb\" ) as res_file:\n",
    "        train_result = pickle.load( res_file )  \n",
    "    thresholds[disc_type] = train_result['pred_thresh']\n",
    "    print(disc_type, train_result)\n",
    "sub = pd.concat([sub_df(predict_strings(disc_type, thresholds[disc_type], uniqueSubmitGroups, submit=True)) for disc_type in disc_type_to_ids ]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "gradient": {
     "execution_count": 29,
     "id": "658e2ab3-b02d-449c-ad21-340e6206586f",
     "kernelId": ""
    },
    "papermill": {
     "duration": 1.020359,
     "end_time": "2021-12-21T12:58:54.413788",
     "exception": false,
     "start_time": "2021-12-21T12:58:53.393429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "gradient": {
     "execution_count": 30,
     "id": "b2bd03cf-0460-446d-9340-71b477c78193",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id                 class  \\\n",
      "0   0FB0700DAF44              Evidence   \n",
      "1   0FB0700DAF44              Evidence   \n",
      "2   0FB0700DAF44              Evidence   \n",
      "3   18409261F5C2              Evidence   \n",
      "4   18409261F5C2              Evidence   \n",
      "5   18409261F5C2              Evidence   \n",
      "6   D46BCB48440A              Evidence   \n",
      "7   D46BCB48440A              Evidence   \n",
      "8   D46BCB48440A              Evidence   \n",
      "9   D72CB1C11673              Evidence   \n",
      "10  D72CB1C11673              Evidence   \n",
      "11  D72CB1C11673              Evidence   \n",
      "12  DF920E0A7337              Evidence   \n",
      "13  DF920E0A7337              Evidence   \n",
      "14  DF920E0A7337              Evidence   \n",
      "15  0FB0700DAF44                 Claim   \n",
      "16  0FB0700DAF44                 Claim   \n",
      "17  0FB0700DAF44                 Claim   \n",
      "18  0FB0700DAF44                 Claim   \n",
      "19  0FB0700DAF44                 Claim   \n",
      "20  18409261F5C2                 Claim   \n",
      "21  18409261F5C2                 Claim   \n",
      "22  18409261F5C2                 Claim   \n",
      "23  18409261F5C2                 Claim   \n",
      "24  D46BCB48440A                 Claim   \n",
      "25  D46BCB48440A                 Claim   \n",
      "26  D72CB1C11673                 Claim   \n",
      "27  D72CB1C11673                 Claim   \n",
      "28  D72CB1C11673                 Claim   \n",
      "29  DF920E0A7337                 Claim   \n",
      "30  DF920E0A7337                 Claim   \n",
      "31  DF920E0A7337                 Claim   \n",
      "32  DF920E0A7337                 Claim   \n",
      "33  DF920E0A7337                 Claim   \n",
      "34  0FB0700DAF44                  Lead   \n",
      "35  18409261F5C2                  Lead   \n",
      "36  D46BCB48440A                  Lead   \n",
      "37  D72CB1C11673                  Lead   \n",
      "38  DF920E0A7337                  Lead   \n",
      "39  0FB0700DAF44              Position   \n",
      "40  18409261F5C2              Position   \n",
      "41  D46BCB48440A              Position   \n",
      "42  D72CB1C11673              Position   \n",
      "43  DF920E0A7337              Position   \n",
      "44  0FB0700DAF44  Concluding Statement   \n",
      "45  D46BCB48440A  Concluding Statement   \n",
      "46  D72CB1C11673  Concluding Statement   \n",
      "47  DF920E0A7337  Concluding Statement   \n",
      "\n",
      "                                     predictionstring  \n",
      "0   341 342 343 344 345 346 347 348 349 350 351 35...  \n",
      "1   120 121 122 123 124 125 126 127 128 129 130 13...  \n",
      "2   230 231 232 233 234 235 236 237 238 239 240 24...  \n",
      "3   164 165 166 167 168 169 170 171 172 173 174 17...  \n",
      "4   441 442 443 444 445 446 447 448 449 450 451 45...  \n",
      "5   739 740 741 742 743 744 745 746 747 748 749 75...  \n",
      "6   223 224 225 226 227 228 229 230 231 232 233 23...  \n",
      "7   56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 7...  \n",
      "8   150 151 152 153 154 155 156 157 158 159 160 16...  \n",
      "9   94 95 96 97 98 99 100 101 102 103 104 105 106 ...  \n",
      "10  183 184 185 186 187 188 189 190 191 192 193 19...  \n",
      "11  274 275 276 277 278 279 280 281 282 283 284 28...  \n",
      "12  466 467 468 469 470 471 472 473 474 475 476 47...  \n",
      "13  118 119 120 121 122 123 124 125 126 127 128 12...  \n",
      "14  310 311 312 313 314 315 316 317 318 319 320 32...  \n",
      "15  315 316 317 318 319 320 321 322 323 324 325 32...  \n",
      "16  66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 8...  \n",
      "17    49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64  \n",
      "18  84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 9...  \n",
      "19  196 197 198 199 200 201 202 203 204 205 206 20...  \n",
      "20                        155 156 157 158 159 160 161  \n",
      "21                    146 147 148 149 150 151 152 153  \n",
      "22                        139 140 141 142 143 144 145  \n",
      "23  421 422 423 424 425 426 427 428 429 430 431 43...  \n",
      "24                                           44 45 46  \n",
      "25                               36 37 38 39 40 41 42  \n",
      "26                                        74 75 76 77  \n",
      "27                62 63 64 65 66 67 68 69 70 71 72 73  \n",
      "28  83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 9...  \n",
      "29                    100 101 102 103 104 105 106 107  \n",
      "30                            91 92 93 94 95 96 97 98  \n",
      "31                               84 85 86 87 88 89 90  \n",
      "32  458 459 460 461 462 463 464 465 466 467 468 46...  \n",
      "33  302 303 304 305 306 307 308 309 310 311 312 31...  \n",
      "34  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...  \n",
      "35  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...  \n",
      "36  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...  \n",
      "37  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...  \n",
      "38  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...  \n",
      "39                      41 42 43 44 45 46 47 48 49 50  \n",
      "40  124 125 126 127 128 129 130 131 132 133 134 13...  \n",
      "41                   20 21 22 23 24 25 26 27 28 29 30  \n",
      "42  50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 6...  \n",
      "43  68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 8...  \n",
      "44  560 561 562 563 564 565 566 567 568 569 570 57...  \n",
      "45  306 307 308 309 310 311 312 313 314 315 316 31...  \n",
      "46  364 365 366 367 368 369 370 371 372 373 374 37...  \n",
      "47  618 619 620 621 622 623 624 625 626 627 628 62...  \n"
     ]
    }
   ],
   "source": [
    "print(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "id": "0deaf0ca-410b-4e2f-882b-617d1c262662",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
